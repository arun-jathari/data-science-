{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial MSE}{\\partial \\theta_0} = -\\frac{2}{m}\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)$$\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_1} = -\\frac{2}{m}\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)\\big(x^{(i)}\\big)$$  \n",
    "\n",
    "Here $\\theta_0$ is intercept and $\\theta_1$ is slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **model** |  |\n",
    "| --- | ---|\n",
    "| init |initialize the coefficiesnts like intercept and slope|\n",
    "| predict |estimate the output based on the current coefficiesnts values|\n",
    "| get summary |the user friendly output summary|\n",
    "|  |  |\n",
    "|**cost function (objective function)** | it calucaltes the difference between the true and predicated values <br> eg.: MSE, MAE |\n",
    "|  |  |\n",
    "| **optimizer** | |\n",
    "| calculate gradients | it will calculate the gradients of the errors and find out the new deltas of variables <br> eg.: $\\partial$ slope & $\\partial$ intercept values |\n",
    "| perform gradient descent | here the new values of variables will be caluclated|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "<i>hyper params</i>: learning rate $\\eta$, no. of iterations   \n",
    "  \n",
    "new $\\theta_1$ = old $\\theta_1 - \\eta * \\frac{\\partial MSE}{\\partial \\theta_1} $  \n",
    "  \n",
    "new $\\theta_0$ = old $\\theta_0 - \\eta * \\frac{\\partial MSE}{\\partial \\theta_0} $ \n",
    "\n",
    "note: $\\eta$ will decide the stepsize of descent.  \n",
    "&emsp;&emsp;larger $\\eta$ may overshoot the local minima.  \n",
    "&emsp;&emsp;smaller $\\eta$ will take longer time and may not reach the minima\n",
    "\n",
    "**step 1**  \n",
    "calculate the difference of the true and predicted values i.e., $\\displaystyle\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)$\n",
    "   \n",
    "**step 2**  \n",
    "calculate the partial derivatives i.e., $\\frac{\\partial MSE}{\\partial \\theta_1}, \\frac{\\partial MSE}{\\partial \\theta_0}$  \n",
    "  \n",
    "**step 3**  \n",
    "calculate the new $\\theta_0 \\& \\theta_1$ i.e., model's intercept and slope\n",
    "\n",
    "**step 4**  \n",
    "continue the steps from 1 to 3 untill the cost becomes negligible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnSupervised Learning\n",
    "<img src=\"media/UnSupervised Learning.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "<img src=\"media/Supervised Learning.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Objective Function)\n",
    "It helps in maintaining consistant optimization of the model. if cost function always shows smaller errors then optimizer will make smaller changes in the model parameters, if cost function shows larger error for some updates then the optimizer will maker sure that these kind of changes are avoided  \n",
    "e.g.: MSE will throw large error for outliers compared to MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "<img src=\"media/Gradient Descent Graphs.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good and Bad of Data  \n",
    "Is Large amount of data is good?\n",
    "* yes, if it is representative\n",
    "* Doesn't contain errors\n",
    "* Aren't missing key information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representative?  \n",
    "**Sample Data**: it is a subset of the total data which is good enough to express the whole data  \n",
    "&emsp; e.g.: Like Survays made of discrete people and make inferences on all people  \n",
    "**Population Data**: It is every data available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in data?  \n",
    "**Skewed Data**: if majority data collected is favoring one set of outputs then it is called skewed data  \n",
    "&emsp; e.g.: student data contains $90\\%$ girls and $10\\%$ boys  \n",
    "**Measurement error**: Errors due to the Insturment/tool used to collect the data  \n",
    "&emsp; e.g.: rouding errors in data where scale is Billion $\\$$  \n",
    "**Data Entry errors**: Incorrect entry of data at time of data collection  \n",
    "&emsp; e.g.: size of the height of human is recorded as 18mts instead of 1.8mts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Missing Data](https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184)  \n",
    "If there is any missing information in any of the column of the collected data then it is called as Missing Data  \n",
    "which can be imputed by many procedures like mean, mode, regression etc,.  \n",
    "or  \n",
    "remove the samples which are incomplete  \n",
    "or  \n",
    "Use a model which can accept these missing data e.g.: k-Nearest Neighbours, classification and regression trees, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data  \n",
    "### Continuous, ordinal, and categorical data  \n",
    "Continuous data: it is numeric data to which value can be added and deleted  \n",
    "&emsp; e.g.: age, height, income etc,.  \n",
    "  \n",
    "Categorical data: Data containing Labels or categories  \n",
    "&emsp; e.g.: gender, occupation, genre etc,.  \n",
    "  \n",
    "Ordinal data: data which have highrarchical order which can be expressed as sequence of numbers  \n",
    "&emsp; e.g.: rank, ratings, income level etc,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatypes  \n",
    "### Primitive  \n",
    "**integers:** counting numbers, like 2  \n",
    "**floating-point numbers:** numbers with decimal places, like 2.43  \n",
    "**strings:** letters and words  \n",
    "**booleans:** true and false  \n",
    "**None, void, or null:** not data, but rather the absence of it  \n",
    "  \n",
    "### Derivative  \n",
    "Its a combination of one or more primitive datatypes  \n",
    "e.g.: images, dates, 3d models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle these different types of data?  \n",
    "for continuous data, floating point numbers are best  \n",
    "for ordinal data, encode them into integers  \n",
    "for categorical data,  \n",
    "&emsp; if the data contains only two categories, then use either boolean or binary encoding  \n",
    "&emsp; if it has more than two categories, use dummy variables/one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding, data cleaning, and statistical power  \n",
    "before performing we need to know whether what kind of impact it gonna make it can found out by statistical power  \n",
    "Statistical Power is the ability of the model to correctly identify the relations between features and labels  \n",
    "  \n",
    "**removing data:** removing data can lower the statistical power  \n",
    "&emsp; e.g.: skewed data like 99 girls 1 boy  \n",
    "**Unnessary columns:** unnessary columns can reduce the statistical power  \n",
    "&emsp; e.g.: gender information in a performance measurement  \n",
    "**one hot encoding** can also lowers the Stastical power much more than the continuous or categorical data  \n",
    "&emsp; e.g.: column with 200 categories in 1000 row data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy  \n",
    "Its a python library which will provide the functionality to perform comparable mathematical operations like MATLAB and R. It helps user building complex mathematical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas  \n",
    "It is also a python library which helps user to understand the data in tabular formats, in short it is kind of the python version of excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing  \n",
    "Data analytics and visualizations are iterative processess typically  \n",
    "**Data cleaning** and **imputing Missing values** in data  \n",
    "**Applying Statistical techniques** to understand if the sample data abel to represent the whole data  \n",
    "**Data Visualization** are used to understand the realations between various columns  \n",
    "**Hypothesis** is revised and repeat the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers  \n",
    "the data points which are beyound the normal data/reasonable data is known as outliers  \n",
    "some of these outliers can be found easily and for others we need to have the understanding of the data theoritically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias & Variance  \n",
    "These are variations in the data which can significantly effect our model performance, need to take care these data points through central tendency concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression  \n",
    "Advantages:  \n",
    "Predictable and easy to interpret  \n",
    "easy to extrapolate  \n",
    "Optimal solution is usually garanteed\n",
    " - It can find the Optimal solution with garantee if the dataset is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Leanear Regression Assumptions  \n",
    "Every feature considered are assumed to be linearly independent, if not then the result of model can be misleading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit: $R^2$ (Coefficient of Determination)  \n",
    "is the correlation between x and y squared.  \n",
    "Strong correlations means larger $R^2$  \n",
    "$R^2=1$ : pefectly predicts the output  \n",
    "$R^2=0$ : no relation between the variables  \n",
    "  \n",
    "$R^2$ limitations:\n",
    "* Model with high data will have high $R^2$ compared to same model with low data  \n",
    "* Cannot garantees the models prediction power on unseen data  \n",
    "* It cannot tell the direction of relation between variables\n",
    "  \n",
    "In complex models $R^2=0.3$ can be a perfect score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models  \n",
    "### Linear Regression  \n",
    "- it is the simplest of all the regression models  \n",
    "- no limit on the no of features used  \n",
    "- it comes in many forms\n",
    "    - Simple Linear Regression\n",
    "    - Multiple Linear Regression\n",
    "    - Polynomial Regression etc,.\n",
    "- e.g.: ols, Lasso and Ridge regression\n",
    "### Decision Trees  \n",
    "- A step-by-step approch\n",
    "- it splits the data into based on the higher entropy\n",
    "### Ensemble algorithms  \n",
    "- it is a group of multiple trees\n",
    "    - e.g.: Random Forest\n",
    "- its prediction capabilities are strong\n",
    "- it imporves the generality of model\n",
    ">  [Scikit-Learn documentation](https://scikit-learn.org/stable/supervised_learning.html)<br>[Scikit-Learn estimator cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "$$\\boxed{J(\\theta) = \\displaystyle\\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}}$$\n",
    "  \n",
    "$h_{\\theta}(x) = \\theta^{T}x = \\theta_{0} + \\theta_{1}*x$  \n",
    "in terms of matrix multiplication we can do it as  \n",
    "$\\implies \\boxed{h(\\theta) = \\begin{bmatrix} 1 & x_{0} \\\\ 1 & x_{1} \\\\ 1 & x_{2} \\end{bmatrix} \\times \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\end{bmatrix}}$  \n",
    "i.e., $h(\\theta) = DataMatrix \\times Parameters$  \n",
    "  \n",
    "Similarly we can compare the multilple combinatios of these parameters as  \n",
    "$\\begin{bmatrix} 1 & x_{0} \\\\ 1 & x_{1} \\\\ 1 & x_{2} \\end{bmatrix} \\times \\begin{bmatrix} \\theta_{0}^{1} & \\theta_{0}^{2} & \\theta_{0}^{2} \\\\ \\theta_{1}^{1} & \\theta_{1}^{2} & \\theta_{1}^{3} \\end{bmatrix} = \\begin{bmatrix} h^{1}(\\theta) & h^{2}(\\theta) & h^{3}(\\theta) \\end{bmatrix}_{3 \\times 3}$  \n",
    "  \n",
    "**Properties of Matrix Multiplication (In general):**  \n",
    "A x B $\\ne$ B x A (not commutative)  \n",
    "A x (B x C) = (A x B) x C (Associative)  \n",
    "$I_{m \\times m}$ x $A_{m \\times n}$ = $A_{m \\times n}$ x $I_{n \\times n}$ = $A_{m \\times n}$ (Identity matrix)  \n",
    "If A is an (m x m) matrix and if it has an inverse  \n",
    "$A \\times A^{-1}=A^{-1} \\times A=I_{m \\times m}$  \n",
    "&emsp;*Note:* Matrices that don't have an inverse are called as \"singular\" or \"degenerate\"  \n",
    "$A^{T} = B$ (Matrix Transpose)  \n",
    "$\\implies B_{ij} = A_{ji}$\n",
    "  \n",
    "  \n",
    "For Multi variable Linear regression\n",
    "$X = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}_{n+1,1} (where \\space x_{0}=1) ,\\space \\space \\space\n",
    "\\Theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\vdots \\\\ \\theta_{n} \\end{bmatrix}_{n+1, 1}$\n",
    "$\\because \\space h_{\\theta}(x) = \\theta_{0} + \\theta_{1}*x_{1} + \\theta_{2}*x_{2} + \\dots + \\theta_{n}*x_{n}$  \n",
    "  \n",
    "$\\implies h_{\\theta}(x)= \\begin{bmatrix} \\theta_{0} & \\theta_{1} & \\dots & \\theta_{n} \\end{bmatrix} \\times$\n",
    "$\\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}$\n",
    "$=  \\Theta^{T} \\times X$  \n",
    "  \n",
    "here n is number of features  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "new algorithm $(n \\ge 1)$  \n",
    "  \n",
    "Repeat $\\bigg\\{$  \n",
    "$$\\boxed{\\theta_{j} = \\theta_{j} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}$$ \n",
    "(simultaneously update $\\theta_{j}$ for j = 0, $\\dots$, n)  \n",
    "$\\bigg\\}$  \n",
    "where $x_{0}^{(i)}=1$  for (i $\\epsilon$ 1, $\\dots$, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**  \n",
    "* Mean Normalization\n",
    "$$x_{i}:=\\frac{x_{i}-\\mu_{i}}{s_{i}}$$\n",
    "$s_{i} = max(x_{i}) - min(x_{i})$  \n",
    "standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation  \n",
    "  \n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "  \n",
    "here X is of (m x (n+1)) dimension, where first column will be all 1's i.e., $x_{0}^{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Gradient Descent** | **Normal Equation**|  \n",
    "| --- | ---|\n",
    "| Need to choose alpha | No need to choose alpha|  \n",
    "| Needs many iterations | No need to iterate|  \n",
    "|$O(kn^{2})$ | $O(n^{3})$, need to calculate inverse of $X^{T}X$|  \n",
    "| Works well when n is large | Slow if n is very large|\n",
    "|consider $m>10^{6}$| $m<10^{6}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $X^{T}X$ is non invertable  \n",
    "* Redundant features(Linearly dependent)  \n",
    "    e.g.: $x_{1} = 2x_{2}$\n",
    "* Too many features (e.g: $m \\le n$)\n",
    "    * delete some features or use regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function in Multivariate case using vectors Gradient Descent \n",
    "$$\\boxed{J(\\theta) = \\frac{1}{2m}(X\\Theta - y)^{T}(X\\Theta - y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**\n",
    "$$\\boxed{\\Theta = (X^{T}X)^{-1}X^{T}y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression  \n",
    "  \n",
    "$h_{\\theta}(x) = g(\\theta^{T}x)$  \n",
    "  \n",
    "$g(z) = \\displaystyle\\frac{1}{1+e^{-z}}$  \n",
    "  \n",
    "$\\boxed{\\therefore h_{\\theta}(x) = \\displaystyle\\frac{1}{1+e^{-\\theta^{T}x}}}$  \n",
    "  \n",
    "$\\implies 0\\le h_{\\theta}(x)\\le 1$  \n",
    "\n",
    "$\\boxed{h_{\\theta}(x) = P(y=1|_{x};\\theta)}$  \n",
    "\"porbability that y=1, given x, parameterized by $\\theta$\"  \n",
    "  \n",
    "$\\boxed{P(y=0|_{x};\\theta) = 1 - P(y=1|_{x};\\theta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Boundary**  \n",
    "  \n",
    "$h_{\\theta} \\ge 0.5 \\rightarrow y = 1$  \n",
    "$h_{\\theta} < 0.5 \\rightarrow y = 0$  \n",
    "  \n",
    "$\\because g(z) \\ge 0.5$  \n",
    "when $z\\ge 0$  \n",
    "\n",
    "$*$ Note:  \n",
    "$z = 0, e^{0} = 1 \\implies g(z) = 1/2$  \n",
    "$z \\rightarrow \\infin, e^{-\\infin} \\rightarrow 0 \\implies g(z) = 1$  \n",
    "$z \\rightarrow -\\infin, e^{\\infin} \\rightarrow \\infin \\implies g(z) = 0$  \n",
    "  \n",
    "$\\boxed{\\begin{matrix}\\theta^{T}x \\ge 0 \\implies y = 1\\\\\n",
    "\\theta^{T}x < 0 \\implies y = 0\\end{matrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to $g(z)$ in Logistic Regression $\\implies$ non-convex cost function  \n",
    "$\\therefore$ consider following cost function which will result in convex cost function  \n",
    "  \n",
    "$$J(\\theta) = \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)}),y^{(i)})$$    \n",
    "\n",
    "$Cost(h_{\\theta}(x),y) =\\begin{cases} \n",
    "    -log(h_{\\theta}(x)) & \\text{if~~} y=1 \\\\ \n",
    "    -log(1-h_{\\theta}(x)) & \\text{if~~}y=0 \n",
    "\\end{cases}$  \n",
    "  \n",
    "$$\\implies Cost(h_{\\theta}(x),y) = -y\\text{~}log(h_{\\theta}(x))-(1-y)log(1-h_{\\theta}(x))$$  \n",
    "  \n",
    "$$\\implies \\boxed{J(\\theta) = - \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg[y^{(i)}log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\\bigg]}$$  \n",
    "  \n",
    "Vector Form  \n",
    "$\\boxed{\\begin{matrix}J(\\theta) = \\displaystyle\\frac{1}{m}\\bigg[-y^{T}log(h)-(1-y)^{T}log(1-h)\\bigg] \\\\\n",
    "  \\\\  \n",
    "\\text{where } h = g(\\theta^{T}X)\\end{matrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\Linear Regression.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cost(h_{\\theta}(x),y) = 0 \\text{~~if~~} h_{\\theta}(x)=y$  \n",
    "$Cost(h_{\\theta}(x),y) \\rightarrow \\infin \\text{~~if~~}y=0 \\text{~~and~~} h_{\\theta}(x) \\rightarrow 1$  \n",
    "$Cost(h_{\\theta}(x),y) \\rightarrow \\infin \\text{~~if~~}y=1 \\text{~~and~~} h_{\\theta}(x) \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "$J(\\theta) = - \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg[y^{(i)}log\\text{~}h_{\\theta}(x^{(i)}) + (1-y^{(i)})log\\big(1-h_{\\theta}(x^{(i)})\\big)\\bigg]$  \n",
    "  \n",
    "Want $ \\min\\limits_{\\theta}J(\\theta):$\n",
    "  \n",
    "$Repeat \\bigg\\{    \n",
    "\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}}J(\\theta)  \n",
    "\\bigg\\} $\n",
    "  \n",
    "(simultaniously update all $\\theta_{j}$)  \n",
    "  \n",
    "where $\\displaystyle\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) = \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)}$\n",
    "\n",
    "$\\boxed{\\therefore Repeat \\bigg\\{    \n",
    "\\theta_{j}:=\\theta_{j}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)}  \n",
    "\\bigg\\}} $  \n",
    "  \n",
    "where $h_{\\theta}(x) = \\displaystyle\\frac{1}{1+e^{-\\theta^{T}x}}$   \n",
    "  \n",
    "vector form  \n",
    "$$\\boxed{\\theta = \\theta - \\frac{\\alpha}{m}X^{T}\\bigg(g(X^{T}\\theta)-\\overrightarrow{y}\\bigg)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization algorithms**\n",
    "* Gradient descent\n",
    "* Conjugate gradient\n",
    "* BFGS\n",
    "* L-BFGS\n",
    "  \n",
    "Advanced algorithms  \n",
    "* Advantages:\n",
    "    * No need to manually pick $\\alpha$\n",
    "    * Often faster than gradient descent  \n",
    "  \n",
    "* Disadvantages:\n",
    "    * More complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Logistic Regression\n",
    "<img src=\"media\\one-vs-all.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\implies h_{\\theta}^{(i)}(x) = P(y=i|_{x};\\theta)\\text{~~~~~~~~~}(i=1,2,3)$  \n",
    "\"porbability that $y=i$ (class), given $x$, parameterized by $\\theta$\"  \n",
    "  \n",
    "Train a logistic regression classifier $h_{\\theta}^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.  \n",
    "  \n",
    "On a new input $x$, to make a prediction, pick the class $i$ that maximizes  \n",
    "$\\implies \\boxed{prediction = \\operatorname*{max}\\limits_i h_{\\theta}^{(i)}(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Over Fitting**  \n",
    "\n",
    "Types of fitting:  \n",
    "* Under fitting or High Bias\n",
    "* right fit\n",
    "* Over fitting or High Varience\n",
    "\n",
    "How to address Overfitting  \n",
    "1) Reduce the number of features:\n",
    "\n",
    "    * Manually select which features to keep.\n",
    "\n",
    "    * Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2) Regularization\n",
    "\n",
    "    * Keep all the features, but reduce the magnitude of parameters $\\theta_{j}$  \n",
    "  \n",
    "    * Regularization works well when we have a lot of slightly useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\begin{matrix} \\therefore J(\\theta) = \\displaystyle\\frac{1}{2m}\\bigg[\\overbrace{\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)^{2}}^{\\text{cost}}+\\overbrace{\\lambda \\sum_{j=1}^{n}\\theta_{j}^{2}\\bigg]}^{\\text{Regularization Term}} \\\\\n",
    "\\\\\n",
    "\\text{i.e., } \\min\\limits_\\theta J(\\theta) \\text{~~~~~~~(minimizing the cost by minimizing }\\theta) \\\\\n",
    "\\\\\n",
    "\\lambda \\text{ : regularization parameter}\\end{matrix}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\regularization.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "Repeat $\\bigg\\{$  \n",
    "$\\theta_{0}:=\\theta_{0}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{0}^{(i)}$  \n",
    "  \n",
    "$\\theta_{j}:=\\theta_{j}-\\alpha \\bigg[\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg] \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n)$  \n",
    "$\\bigg\\}$\n",
    "  \n",
    "the second eq'n also writen as  \n",
    "$\\implies \\theta_{j}:=\\theta_{j}\\underbrace{\\bigg(1-\\alpha\\displaystyle\\frac{\\lambda}{m}\\bigg)}_{<1}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} \n",
    " \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**  \n",
    "$\\boxed{\\theta = \\Biggr(X^{T}X + \\lambda\\begin{bmatrix} 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & 1 \\end{bmatrix}_{n+1,n+1}\\Biggr)^{-1}X^{T}y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non invertable**   \n",
    "if $m\\le n$    \n",
    "$\\implies \\theta \\neq (\\underbrace{X^{T}X}_{\\text{non-invertable}})^{-1}X^{T}y$  \n",
    "  \n",
    "$\\because X^T X$ is non invertable  \n",
    "  \n",
    "But if $\\lambda > 0$  \n",
    "$\\theta = \\Biggr(\\underbrace{X^{T}X + \\lambda\\begin{bmatrix} 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & 1 \\end{bmatrix}_{n+1,n+1}}_{\\text{invertable}}\\Biggr)^{-1}X^{T}y$  \n",
    "i.e., regularization overcomes the singularity problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with regularization  \n",
    "  \n",
    "$J(\\theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^n\\theta_j^2$  \n",
    "\n",
    "**Gradient Descent**: Logistic Regression gradient descent is same as Linear Regression  \n",
    "Repeat $\\bigg\\{$  \n",
    "$\\theta_{0}:=\\theta_{0}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{0}^{(i)}$  \n",
    "  \n",
    "$\\theta_{j}:=\\theta_{j}-\\alpha \\bigg[\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg] \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n)$  \n",
    "$\\bigg\\}$\n",
    "  \n",
    "the second eq'n also writen as  \n",
    "$\\implies \\theta_{j}:=\\theta_{j}\\underbrace{\\bigg(1-\\alpha\\displaystyle\\frac{\\lambda}{m}\\bigg)}_{<1}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} \n",
    " \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "<img src=\"media\\neural network.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in simpler terms  \n",
    "$[x_0x_1x_2x_3] \\rightarrow [a_1^{(2)}a_2^{(2)}a_3^{(2)}] \\rightarrow h_{\\theta}(x)$  \n",
    "  \n",
    "$a_i^{(j)} = $\"activation\" of unit $i$ in layer $j$  \n",
    "$\\Theta^{(j)} = $ matrix of weights controlling function mapping  from layer $j$ to layer $j+1$  \n",
    "  \n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$  \n",
    "  \n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$  \n",
    "  \n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$  \n",
    "  \n",
    "$h_{\\Theta}(x) = a_1^{(3)} = g(\\Theta^{(2)}_{10}a_{0}^{(2)} + \\Theta^{(2)}_{11}a_{1}^{(2)} + \\Theta^{(2)}_{12}a_{2}^{(2)} + \\Theta^{(2)}_{13}a_{3}^{(2)})$  \n",
    "  \n",
    "If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let  \n",
    "  \n",
    "$a_1^{(2)} = g(z_1^{(2)})$  \n",
    "  \n",
    "$a_2^{(2)} = g(z_2^{(2)})$  \n",
    "  \n",
    "$a_3^{(2)} = g(z_3^{(2)})$  \n",
    "  \n",
    "i.e., $z_k^{(2)} = \\Theta_{k,0}^{(1)}x_0 + \\Theta_{k,1}^{(1)}x_1 + \\dots + \\theta_{k,n}^{(1)}x_n$  \n",
    "  \n",
    "in vector form  \n",
    "$x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}$    $z^{(j)} = \\begin{bmatrix} z_0^{(j)} \\\\ z_1^{(j)} \\\\ \\dots \\\\ z_n^{(j)} \\end{bmatrix}$    \n",
    "  \n",
    "$\\boxed{\\therefore a^{(j)} = g(z^{(j)}) \\impliedby  z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g., Build XNOR Gate**  \n",
    "the $\\Theta^{(1)}$ matrices for AND, NOR and OR are:  \n",
    "$AND: \\Theta^{(1)} = \\begin{bmatrix} -30 \\\\ 20 \\\\ 20 \\end{bmatrix} \\text{,~~~ }\n",
    "NOR: \\Theta^{(1)} = \\begin{bmatrix} 10 \\\\ -20 \\\\ -20 \\end{bmatrix} \\text{,~~~~ }\n",
    "OR: \\Theta^{(1)} = \\begin{bmatrix} -10 \\\\ 20 \\\\ 20 \\end{bmatrix}$  \n",
    "$\\text{\\\\ \\\\}$\n",
    "$$\\implies \\underbrace{\\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix}}_{L1} \\xRightarrow{\\Theta^{(1)}} \\underbrace{\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix}}_{L2} \\xRightarrow{\\Theta^{(2)}} \\underbrace{\\begin{bmatrix} a^{(3)} \\end{bmatrix}}_{L3} \\rightarrow h_{\\Theta}(x)$$  \n",
    "$\\text{\\\\ \\\\}$  \n",
    "$\\text{where ~~~~~~} \\Theta^{(1)} = \\begin{bmatrix} -30 & 10 \\\\ 20 & -20 \\\\ 20 & -20 \\end{bmatrix} \n",
    "\\text{~~~~~~~~~~ \\& ~~~~~~~~~} \n",
    "\\Theta^{(2)} = \\begin{bmatrix} -10 \\\\ 20 \\\\ 20 \\end{bmatrix}$ \n",
    "$\\Downarrow$\n",
    "|| dimensions|\n",
    "|--|--|\n",
    "|$a^{(2)} = g(\\Theta^{(1)}.x)$ | $(2\\times 1) = (3\\times 2)^T*(3\\times 1)$|\\\\\n",
    "|$$\\downarrow$$| bias term $a_0^{(2)} = 1$ is added to $a^{(2)}$ i.e.,$ (2\\times 1)\\rightarrow (3\\times 1)$|\n",
    "|$a^{(3)} = g(\\Theta^{(2)}.a^{(2)})$| $(1\\times 1) = (3\\times 1)^T*(3\\times 1)$|\n",
    "|$$\\downarrow$$| |\n",
    "|$h_{\\Theta}(x) = a^{(3)}$| $(1\\times 1)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification (neural network)\n",
    "lets consider the four possible outcomes as y:\n",
    "$\\\\y^{(i)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\ \\\\$\n",
    "\n",
    "$NN \\implies \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(2)} \\\\ a_1^{(2)} \\\\ a_2^{(2)} \\\\ \\dots \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(3)} \\\\ a_1^{(3)} \\\\ a_2^{(3)} \\\\ \\dots \\end{bmatrix} \\rightarrow \\dots \\rightarrow \\begin{bmatrix} h_{\\Theta}(x)_1 \\\\ h_{\\Theta}(x)_2 \\\\ h_{\\Theta}(x)_3 \\\\ h_{\\Theta}(x)_4 \\end{bmatrix} \\\\ \\\\$  \n",
    "  \n",
    "i.e., if $h_{\\Theta}(x) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0  \\end{bmatrix} \\implies $ output as class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\4L neural network.drawio.svg\">  \n",
    "  \n",
    "  Fig: 4 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let training set be:  \n",
    "    \n",
    "$\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$  \n",
    "$L = $ total no. of layers $(l)$ in network  \n",
    "$s_l = $ no. of units(not counting bias unit) in layer $l$   \n",
    "i.e., in above nn $s_1 = 3, s_2 = 5, s_3 = 5, s_4 = s_L = 4$  \n",
    "  \n",
    "**Binary Classification:**  \n",
    "$y =$ 0 or 1  \n",
    "i.e., 1 output unit  \n",
    "  \n",
    "**Multi-class classification ($K$ classes)**  \n",
    "$y \\text{ }\\large{\\epsilon} \\text{ } \\mathbb{R}^K$  \n",
    "e.g., $\\underbrace{\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{pedestrain}, \\underbrace{\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{car}, \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}}_{motorcycle}, \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}}_{truck}$  \n",
    "  \n",
    "$K$ output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function  \n",
    "### Logistic Regression  \n",
    "$\\large{J(\\theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^n\\theta_j^2}$  \n",
    "  \n",
    "### Neurla network:  \n",
    "$$\\large{h_{\\Theta}(x) \\text{ } \\Large{\\epsilon} \\text{ } \\mathbb{R}^{K} \\text{~~~~~} (h_{\\Theta}(x))_i = i^{th} output}$$  \n",
    "$\\large{J(\\Theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\sum_{k=1}^K \\bigg(y_k^{(i)}\\log(h_{\\Theta}(x^{(i)}))_k + (1-y_k^{(i)}) \\log(1-h_{\\Theta}(x^{(i)}))_k\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{l=1}^{L-1}\\displaystyle\\sum_{i=1}^{s_l}\\displaystyle\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propogation Algorithm  \n",
    "  \n",
    "training set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$  \n",
    "* set $\\Delta^{(l)}_{i,j} := 0 \\text{ }\\forall (l,i,j)$  \n",
    "  \n",
    "for t = 1 to m:    \n",
    "1. set $a^{(1)} := x^{(t)}$  \n",
    "2. compute $a^{(l)}$ for $l=2,3,...,L$  \n",
    "    * for one training example $(x,y)$\n",
    "      * $a^{(1)} = x$  \n",
    "      * $z^{(2)} = \\Theta^{(1)}a^{(1)}$  \n",
    "      * $a^{(2)} = g(z^{(2)})  \\text{~~~~~~~(add } a_0^{(2)})$\n",
    "      * $z^{(3)} = \\Theta^{(2)}a^{(2)}$  \n",
    "      * $a^{(3)} = g(z^{(3)})  \\text{~~~~~~~(add } a_0^{(3)})$\n",
    "      * $z^{(4)} = \\Theta^{(3)}a^{(3)}$  \n",
    "      * $a^{(4)} = h_{\\Theta}(x) = g(z^{(2)})$\n",
    "3. $\\delta^{(L)} = a^{(L)} - y^{(t)}$\n",
    "4. Compute $\\delta^{(L-1)}, \\delta^{(L-2)},...,\\delta^{(2)}$ using $\\large{\\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*\\underbrace{z^{(l)}.*(1-z^{(l)})}_{g'(z^{(l)})}}  \\text{~~~~~~~~~~(add } z_0^{(l)} \\text{ to } z^{(l)}) $  \n",
    "$\\because g'(x) = x.*(1-x)$  \n",
    "  \n",
    "5. $\\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)}\\delta_i^{(l+1)}$, in vector form $\\Delta^{(l)} := \\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T$  \n",
    "  \n",
    "final $\\Delta$ matrix will be:  \n",
    "  * $D_{i,j}^{(l)}:= \\displaystyle\\frac{1}{m}(\\Delta_{i,j}^{(l)}+\\lambda\\Theta_{i,j}^{(l)}),$ if $j\\ne0$  \n",
    "  * $D_{i,j}^{(l)}:= \\displaystyle\\frac{1}{m}\\Delta_{i,j}^{(l)}$ if $j=0$  \n",
    "  \n",
    "i.e., $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = D_{i,j}^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Learning algorithm  \n",
    "* Get more training examples\n",
    "* Try smaller sets of features\n",
    "* Try getting additional features\n",
    "* Try adding polynomial features $(x_1^2, x_2^2, x_1x_2,..ets.)$  \n",
    "* Try decreasing $\\lambda$\n",
    "* Try increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into 3 sets\n",
    "* Training set $\\text{~~~~~~~~~~~~~~~~~~~~~~}x_{train},y_{train}$\n",
    "* Cross-validation set $\\text{~~~~~~~~~~}x_{cv},y_{cv}$\n",
    "* Test set $\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~}x_{test},y_{test}$\n",
    "\n",
    "in general its ratio is 60% - 20% - 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test set error\n",
    "  \n",
    "$J_{test}(\\Theta) = \\displaystyle\\frac{1}{2m_{test}}\\displaystyle\\sum_{i=1}^{m_{test}}(h_{\\Theta}(x_{test}^{(i)})-y_{test}^{(i)})^{2}$\n",
    "  \n",
    "$err(h_{\\theta}(x),y) =\\begin{cases} \n",
    "    1 & \\begin{cases} \\text{if~~} h_{\\theta}(x) \\ge 0.5 \\text{ \\& } y=0 \\\\ & \\text{or} \\\\ \\text{if ~}h_{\\theta}(x) < 0.5 \\text{ \\& } y=1\\end{cases}\\\\ \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}$  \n",
    "  \n",
    "  \n",
    "$\\text{Test Error} = \\displaystyle\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}err(h_{\\theta}(x_{test}^{(i)}),y_{test}^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection (degree of polynomial)\n",
    "$\\begin{matrix} d=1 & h_{\\theta}(x) = \\theta_0 + \\theta_1x & \\rightarrow \\theta^{(1)} \\rightarrow J_{test}(\\theta^{(1)})\\\\\n",
    "d=2 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 & \\rightarrow \\theta^{(2)} \\rightarrow J_{test}(\\theta^{(2)})\\\\\n",
    "d=3 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\dots + \\theta_3x^3 & \\rightarrow \\theta^{(3)} \\rightarrow J_{test}(\\theta^{(3)})\\\\\n",
    "\\vdots & &\\\\\n",
    "d=10 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\dots + \\theta_{10}x^{10} & \\rightarrow \\theta^{(10)} \\rightarrow J_{test}(\\theta^{(10)})\n",
    "\\end{matrix}$\n",
    "  \n",
    "$d$ is degree of polynomial\n",
    "\n",
    "tune the $d$ parameter using the cross validataion dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\bias-variance.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try different values of $\\lambda \\text{~~}\\epsilon \\text{~~}\\{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\\}$  \n",
    "tune $\\lambda$ using cross validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "$J_{train}(\\Theta) = \\displaystyle\\frac{1}{2m_{train}}\\displaystyle\\sum_{i=1}^{m_{train}}(h_{\\Theta}(x_{train}^{(i)})-y_{train}^{(i)})^{2}$  \n",
    "  \n",
    "$J_{cv}(\\Theta) = \\displaystyle\\frac{1}{2m_{cv}}\\displaystyle\\sum_{i=1}^{m_{cv}}(h_{\\Theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\Learning curves.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Bias**:  \n",
    "* **Low training set size:** causes $J_{train}(\\Theta)$ to be low and $J_{CV}(\\Theta)$ to be high\n",
    "* **Large training set size:** causes $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be high with $J_{train}(\\Theta)\\approx J_{CV}(\\Theta)$  \n",
    "  \n",
    "If a learning algorithm is suffering from **high bias**, getting more training data will not (**by itself**) help much.  <br><br>\n",
    "  \n",
    "**High Varience**: \n",
    "* **Low training set size:** $J_{train}(\\Theta)$ will be low and $J_{CV}(\\Theta)$ will be high\n",
    "* **Large training set size**: $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\\Theta) < J_{CV}(\\Theta)$ but the difference between them remains significant.\n",
    "  \n",
    "If a learning algorithm is suffering from **high variance**, getting more training data is likely to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "bl { color: blue}\n",
    "</style>\n",
    "\n",
    "## $\\therefore$\n",
    "* Get more training examples $\\rightarrow$ <g>fixes high varience</g>\n",
    "* Try smaller sets of features $\\rightarrow$ <g>fixes high varience</g>\n",
    "* Try getting additional features $\\rightarrow$ <g>fixes high bias</g>\n",
    "* Try adding polynomial features $(x_1^2, x_2^2, x_1x_2,..ets.)$  $\\rightarrow$  <g>fixes high bias</g>\n",
    "* Try decreasing $\\lambda$ $\\rightarrow$  <g>fixes high bias</g>\n",
    "* Try increasing $\\lambda$ $\\rightarrow$  <g>fixes high varience</g>\n",
    "  \n",
    "**Effects of no. of parameters** (no. of layers or no. of neurons) \n",
    "* A neural network with <bl>fewer parameters</bl> is **likely to underfit**. It also **computationally cheaper**\n",
    "* A neural network with <bl>many parameters</bl> is **likely to overfit**. which is **computationally expensive**. using <g>lower values of $\\lambda$ may address the issue</g>\n",
    "  \n",
    "**Effects of model complexity**\n",
    "* **Low order polynomials** (low model complexity) have <r>high bias</r> and <r>low variance</r>. In this case model fits <bl>poorly consistently</bl>.\n",
    "* **Higher-order polynomials** (high model complexity) <g>fit training data</g> <bl>extremely well</bl> and the <g>test data</g> <bl>extremely poor</bl>.   i.e., <r>low bias</r> & <r>high variance</r> on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative view of logistic regression  \n",
    "Cost of example:   \n",
    "$\\text{~~~~~~~~~~~~~~~~}-(y\\log h_{\\theta}(x)+(1-y)\\log(1-h_{\\theta}(x)))$  \n",
    "  \n",
    "$\\text{~~~~~~~~~~~~~~~~}=-y\\log\\displaystyle\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)\\log(1-\\displaystyle\\frac{1}{1+e^{-\\theta^Tx}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\log function.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "bl { color: blue}\n",
    "</style>\n",
    "# Support Vector machine\n",
    "<bl>Logistic regression:</bl>  \n",
    "  \n",
    "$\\min\\limits_{\\theta}\\displaystyle\\frac 1m\\bigg[\\sum_{i=1}^{m}y^{(i)}\\underbrace{\\bigg(-\\log h_{\\theta}(x^{(i)})\\bigg)}_{Cost_1(\\theta^Tx^{(i)})}+(1-y^{(i)})\\underbrace{\\bigg((-\\log(1-h_{\\theta}(x^{(i)}))\\bigg)}_{Cost_0(\\theta^Tx^{(i)})}\\bigg]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$  \n",
    "  \n",
    "<bl>Support vector machine:</bl>  \n",
    "   \n",
    "eleminating constant <r>$m$</r>  \n",
    "$\\min\\limits_{\\theta}\\displaystyle\\xcancel{\\frac{1}{m}}\\bigg[\\sum_{i=1}^{m}y^{(i)}Cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\\theta^Tx^{(i)})\\bigg]+\\frac{\\lambda}{2\\xcancel m}\\sum_{j=1}^n\\theta_j^2$  \n",
    "$\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\downarrow\\downarrow\\downarrow$  \n",
    "  \n",
    "also eleminating <r>$\\lambda$</r> as:  \n",
    "$\\min\\limits_{\\theta}$ A+$\\lambda$ B $\\text{~~~~~~~~~~~}$ can be writtent as $\\text{~~~~~~~~~~~}$ $\\min\\limits_{\\theta}$ CA+B  \n",
    "where $C \\propto \\displaystyle\\frac 1\\lambda$  \n",
    "  \n",
    "$\\implies \\min\\limits_{\\theta}C\\displaystyle\\bigg[\\sum_{i=1}^{m}y^{(i)}Cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\\theta^Tx^{(i)})\\bigg]+\\frac{1}{2}\\sum_{j=1}^n\\theta_j^2$  \n",
    "  \n",
    "<bl>SVM hypothesis</bl>  \n",
    "  \n",
    "$h_{\\theta}(x) = \\begin{cases} 1 & \\text{if~~} \\theta^Tx \\geq 0\\\\\n",
    "                                0 & \\text{otherwise}\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\SVM.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Decision Boundary with outliers\n",
    "<img src='media\\svm with outliers.drawio.svg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "Unlabeled dataset  \n",
    "$x^{(1)},x^{(2)},...,x^{(m)} \\epsilon \\mathbb{R}^{10000}\\\\ \\text{} \\\\ \n",
    "\\downarrow \\text{PCA}\\\\ \\text{} \\\\ \n",
    "z^{(1)},z^{(2)},...,z^{(m)} \\epsilon \\mathbb{R}^{1000}$\n",
    "\n",
    ">Note: Mapping $x^{(i)} \\rightarrow z^{(i)}$ should be defined by running PCA on the training set only. only after defining it should be applied on examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in the cross-validation and test datasets\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "- Compute \"covariance matrix\"<br><br>\n",
    "$\\Sigma = \\displaystyle\\frac 1m\\sum_{i=1}^n(x^{(i)})(x^{(i)})^T$<br><br>\n",
    "\n",
    "- Compute \"eigenvectors\" of matrix $\\Sigma$:<br><br>\n",
    "$[U, S, V] = \\text{svd(}\\Sigma)$\n",
    "<br>\n",
    "$U = \\begin{bmatrix} | & | &  \\text{} & |\\\\\n",
    "u^{(1)} & u^{(2)} & ... & u^{(n)}\\\\\n",
    "| & | &  \\text{} & |\\end{bmatrix} \\text{~~} \\Large{\\epsilon} \\text{~~} \\mathbb{R}^{n\\times n}$\n",
    "  \n",
    "- $U_{reduce}= U(:,1:k)$\n",
    "<br><br>\n",
    "- Compute $Z$ matrix:\n",
    "$z^{(i)}_{k\\times 1} = U_{reduce}^T\\times X^{(i)} = {\\underbrace{\\begin{bmatrix} | & | &  \\text{} & |\\\\\n",
    "u^{(1)} & u^{(2)} & ... & u^{(k)}\\\\\n",
    "| & | &  \\text{} & |\\end{bmatrix}}_{n\\times k}}^T\\times X^{(i)}_{n\\times 1} $\n",
    "<br>\n",
    "where<br><br>\n",
    "$U_{reduce} \\text{~~} \\Large{\\epsilon} \\text{~~} \\mathbb{R}^{n\\times k}$ and $k$ is no. of Principal Components <br><br>\n",
    "$X^{(i)} \\text{~~} \\Large{\\epsilon} \\text{~~} \\mathbb{R}^{n\\times 1}$<br><br>\n",
    "$Z^{(i)} \\text{~~} \\Large{\\epsilon} \\text{~~} \\mathbb{R}^{k\\times 1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction of $X_{m\\times n}$\n",
    "$X_{approx}^{(i)} = U_{reduce}\\times Z^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing $k$ (number of principal components)\n",
    "**Algorithm:**  \n",
    "  \n",
    "**Try PCA with** k = 1,...,n   \n",
    "  \n",
    "**Compute** $U_{reduce},z^{(1)},z^{(2)},...,z^{(m)}, x_{approx}^{(1)},...,x_{approx}^{(m)}$  \n",
    "  \n",
    "***Check if***  \n",
    "  \n",
    "$\\displaystyle\\frac{\\frac 1m \\sum_{i=1}^m\\|x^{(i)}-x_{approx}^{(i)}\\|^2}{\\frac 1m \\sum_{i=1}^m\\|x^{(i)}\\|^2}\\leq 0.01?$<br>\n",
    "  \n",
    "$[U, S, V]  = \\text{svd(Sigma)}$  \n",
    "  \n",
    "where $S = \\begin{bmatrix} S_{11} & 0 & \\dots & 0 \\\\\n",
    "0 & S_{22} & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & S_{nn} \\end{bmatrix}$  \n",
    "  \n",
    "The above in-equation can be written in terms of $S$ as: \n",
    "   \n",
    "$1-\\displaystyle\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\leq 0.01$   \n",
    "  \n",
    "$\\implies \\boxed{\\displaystyle\\frac{\\sum_{i=1}^kS_{ii}}{\\sum_{i=1}^nS_{ii}} \\geq 0.99}$  \n",
    "  \n",
    "i.e., $99\\%$ varience retained  \n",
    "> Hint: Pick smallest value of $k$ for which above eqn holds true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of PCA\n",
    "- Compression\n",
    "    - Reduce memory\n",
    "    - speed up the learning algorithms\n",
    "<br><br>\n",
    "- Visualization \n",
    "    - easy to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions on PCA\n",
    "- Do the feature scaling/normalization before hand as there will be square terms in PCA implementation\n",
    "- Do not use it if the model is overfitting instesd use the regularization\n",
    "- Before implementing the PCA try building model without PCA as well so you can have something to compare with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Distribution example\n",
    "<img src='media\\Gaussian distribution.drawio.svg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "1) Choose features $x_i$ that you think might be indicative of anomalous examples.\n",
    "2) Fit parameters $\\mu_1, ..., \\mu_n, \\sigma_1^2, ..., \\sigma_n^2$<br>\n",
    "$\\mu_j = \\displaystyle\\frac 1m \\sum_{i=1}^m x_j^{(i)}$<br>\n",
    "$\\sigma_j^2 = \\displaystyle\\frac 1m \\sum_{i=1}^m(x_j^{(i)}-\\mu_j)^2$\n",
    "3) Given new examples $x$, compute $p(x)$:<br>\n",
    "$p(x) = \\displaystyle\\prod_{j=1}^n p(x_j;\\mu_j,\\sigma_j^2) = \\prod_{j=1}^n\\underbrace{\\frac{1}{\\sqrt{2\\pi}\\sigma_j} \\text{exp}(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})}_{\\text{Gaussian distribution}}$<br>\n",
    "Anomaly if $p(x) < \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulation\n",
    "Fit model $p(x)$ on training set $\\{x^{(1)}, ..., x^{(m)}\\}$ assuming all are non anomalous i.e., $y=0$  \n",
    "On a cross validation/test examples $x$, predict  \n",
    "$y=\\begin{cases} 1 & \\text{if } p(x) < \\varepsilon \\text{ (anomaly)}\\\\\n",
    "0 & \\text{if } p(x) \\geq \\varepsilon \\text{ (normal)}\\end{cases}$\n",
    "Possible evaluation metrics:  \n",
    "- True positive, false positives, false negatives, true negative\n",
    "- Precision/Recall\n",
    "- $F_1$-score\n",
    "    \n",
    "Can also use cross validation set to choose parameter $\\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Anomaly detection**|**Supervised learning**|\n",
    "|---|---|\n",
    "|- Very small number of positive examples $(y=1)$ <br> &emsp; i.e., $m_{y=1}\\approxeq 0 \\text{ to } 20$<br>- Large number of negative $(y=0)$ examples|Large number or positive and negative examples|\n",
    "|Many different \"types\" of anomalies. Hard for any algorithm to learn from positive examples what the anomalies look like<br><br> future anomalies may look nothing like any of the anomalous examples we've seen so far.|Enough positive examples for algorithm to get a sense of   what positive examples are like, future positive examples likely to be similar to ones in training set.|\n",
    "|e.g.,<br>Fraud detection <br>Manufacturing(e.g. aircraft engines)<br>Monitoring machines in a data center<br>$\\vdots$|<br>Email spam classification <br>Weather prediction(sunny/rainy/etc).<br>Cancer classification<br>$\\vdots$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# power transform on data with a skew\n",
    "from numpy import exp\n",
    "from numpy.random import randn\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# generate gaussian data sample\n",
    "data = randn(1000)\n",
    "# add a skew to the data distribution\n",
    "data = exp(data)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(15,5))\n",
    "ax[0].hist(data, bins=25)\n",
    "ax[0].set_title('skewed data')\n",
    "data = data.reshape((len(data),1))\n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "data_trans = power.fit_transform(data)\n",
    "ax[1].hist(data_trans, bins=25)\n",
    "ax[1].set_title('power transformation by yeo-johnson')\n",
    "plt.show()\n",
    "# credits https://machinelearningmastery.com/power-transforms-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Gaussian (Normal) distribution\n",
    "\n",
    "Given training set $\\{x^{(1)}, x^{(2)}, ..., x^{(m)}\\}$  \n",
    "1. Fit model $p(x)$ by setting  \n",
    "$\\mu = \\displaystyle\\frac 1m \\sum_{i=1}^mx^{(i)}$  \n",
    "$\\Sigma = \\displaystyle\\frac 1m \\sum_{i=1}^m(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T$  \n",
    "where  \n",
    "$\\mu \\text{~~} \\large\\epsilon \\text{~~} \\mathbb{R}^n$  \n",
    "$\\Sigma \\text{~~} \\large\\epsilon \\text{~~} \\mathbb{R}^{n\\times n}$  \n",
    "<br>\n",
    "2. Given a new example $x$, compute  \n",
    "$p(x;\\mu, \\Sigma) = \\displaystyle\\frac{1}{(2\\pi)^{\\frac n2}|\\Sigma|^{\\frac 12}}\\text{exp}\\bigg(-\\frac 12 (x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$  \n",
    "Flag an anomaly if $p(x) < \\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate gaussian distribution\n",
    "<img src='media\\Multivariate Gaussian dist.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship to original model \n",
    "Original model: $p(x) = p(x_1;\\mu_1,\\sigma_1^2)\\times p(x_2;\\mu_2,\\sigma_2^2)\\times\\dots\\times p(x_n;\\mu_n,\\sigma_n^2)$  \n",
    "  \n",
    "Corresponds to multivariate Gaussian  \n",
    "$p(x;\\mu, \\Sigma) = \\displaystyle\\frac{1}{(2\\pi)^{\\frac n2}|\\Sigma|^{\\frac 12}}\\text{exp}\\bigg(-\\frac 12 (x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$  \n",
    "  \n",
    "$\\text{where ~~~~~~~~~} \\Sigma = \\begin{bmatrix} \\sigma_1^2 & 0 & \\dots & 0 \\\\\n",
    "0 & \\sigma_2^2 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & \\sigma_n^2 \\end{bmatrix}\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Original Model|Multivariate Gaussian|\n",
    "|---|---|\n",
    "|$p(x) = p(x_1;\\mu_1,\\sigma_1^2)\\times\\dots\\times p(x_n;\\mu_n,\\sigma_n^2)$|$p(x;\\mu, \\Sigma) = \\displaystyle\\frac{1}{(2\\pi)^{\\frac n2}\\|\\Sigma\\|^{\\frac 12}}\\text{exp}\\bigg(-\\frac 12 (x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\bigg)$|\n",
    "|Manually create features to capture anomalies<br> where $x_1, x_2$ take unusual combinations of values.<br>e.g., $x_3 = \\displaystyle\\frac{x_1}{x_2}$|Automatically captures correlations between features|\n",
    "|Computationally cheaper (alternatively, <br>scales better to large n)<br>e.g., n=10,000 to 100,000|Computationally expensive|\n",
    "|OK even if $m$(training set size) is small|Must have $m>n$, or else $\\Sigma$ is non-invertable.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Check for singularity*** $\\Sigma$\n",
    "* check if $m\\geq n$\n",
    "* check if any redudant features like $x_1 = x_2$ or $x_3 = x_4+x_5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Based Recommender system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Motivation\n",
    "|Movie|Alice(1)|Bob(2)|Carol(3)|Dave(4)|$\\operatorname*{x_1}\\limits_{(romance)}$|$\\operatorname*{x_2}\\limits_{(action)}$|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Love at last|5|5|0|0|0.9|0|\n",
    "|Romance forever|5|?|?|0|1.0|0.001|\n",
    "|Cute puppies of love|?|4|0|?|0.99|0|\n",
    "|Nonstop car chases|0|0|5|4|0.1|1.0|\n",
    "|Swords vs. karate|0|0|5|?|0|0.9|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem formulation\n",
    "$r(i,j)  = 1$ if user $j$ has rated movie $i$ (0 otherwise)  \n",
    "$y^{(i,j)} = $ rating by user $j$ on movie $i$ (if defined)  \n",
    "<br>  \n",
    "$\\theta^{(j)} = $ parameter vector for user $j$  \n",
    "$x^{(i)} = $ feature vector for movie $i$  \n",
    "For user $j$, movie $i$, predicted rating: $(\\theta^{(j)})^T(x^{(i)})$  \n",
    "<br>  \n",
    "$m^{(j)} = $ no. of movies rated by user $j$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To learn $\\theta^{(j)}$ parameter for user $j$:**  \n",
    "$\\min\\limits_{\\theta^{(j)}} \\displaystyle\\frac{1}{2m^{(j)}}\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\\bigg)^2 + \\frac{\\lambda}{2m^{(j)}}\\sum_{k=1}^n(\\theta_k^{(j)})^2$\n",
    "<br><br>\n",
    "$\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~}\\downarrow\\downarrow\\downarrow$\n",
    "<br>  \n",
    "$\\min\\limits_{\\theta^{(j)}} \\displaystyle\\frac{1}{2\\xcancel{m^{(j)}}}\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\\bigg)^2 + \\frac{\\lambda}{2\\xcancel{m^{(j)}}}\\sum_{k=1}^n(\\theta_k^{(j)})^2$\n",
    "<br><br>\n",
    "**To learn $\\theta^{(1)},\\theta^{(2)},...\\theta^{(n_u)}$:**  \n",
    "<br>\n",
    "$\\min\\limits_{\\theta^{(j)},...,\\theta^{(n_u)}} \\underbrace{\\displaystyle\\frac{1}{2}\\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^T(x^{(i)})-y^{(i,j)}\\bigg)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n(\\theta_k^{(j)})^2}_{J(\\theta^{(1)},...,\\theta^{(n_u)})}$\n",
    "<br><br>\n",
    "where  \n",
    "$\\theta^{(j)} \\text{~~} \\large\\epsilon \\text{~~} \\mathbb{R}^{n+1}$   \n",
    "<br><br>\n",
    "Gradient descent update:  \n",
    "$\\Bigg\\{$  \n",
    "$$\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha\\displaystyle\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)x_k^{(i)} \\text{~~~(for } k=0)$$\n",
    "$$\\theta_k^{(j)} := \\theta_k^{(j)} - \\alpha\\underbrace{\\Bigg(\\displaystyle\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)x_k^{(i)}+\\lambda\\theta_k^{(j)}\\Bigg)}_{\\frac{\\partial}{\\partial \\theta_k^{(j)}}J(\\theta^{(1)},...,\\theta^{(n_u)})} \\text{(for } k \\neq 0)$$\n",
    "$\\Bigg\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Motivation\n",
    "|Movie|Alice(1)|Bob(2)|Carol(3)|Dave(4)|$\\operatorname*{x_1}\\limits_{(romance)}$|$\\operatorname*{x_2}\\limits_{(action)}$|\n",
    "|---|---|---|---|---|---|---|\n",
    "|Love at last|5|5|0|0|?|?|\n",
    "|Romance forever|5|?|?|0|?|?|\n",
    "|Cute puppies of love|?|4|0|?|?|?|\n",
    "|Nonstop car chases|0|0|5|4|?|?|\n",
    "|Swords vs. karate|0|0|5|?|?|?|\n",
    "\n",
    "also given\n",
    "<br>  \n",
    "$\\theta^{(1)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0\\end{bmatrix}\\text{~~}\\theta^{(2)} = \\begin{bmatrix} 0 \\\\ 5 \\\\ 0\\end{bmatrix}\\text{~~}\\theta^{(3)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5\\end{bmatrix}\\text{~~}\\theta^{(4)} = \\begin{bmatrix} 0 \\\\ 0 \\\\ 5\\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization algorithm (collaborative filtering)\n",
    "**Given $\\theta^{(1)},...,\\theta^{(n_u)}$, to learn $x^{(i)}$:**  \n",
    "$\\min\\limits_{x^{(i)}}\\displaystyle\\frac 12\\sum_{j:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)^2+\\frac{\\lambda}{2}\\sum_{k=1}^n(x_k^{(i)})^2$  \n",
    "<br>  \n",
    "**Given $\\theta^{(1)},...,\\theta^{(n_u)}$, to learn $x^{(1)},...,x^{(n_m)}$:**   \n",
    "$\\min\\limits_{x^{(1)},...,x^{(n_m)}}\\displaystyle\\frac 12\\sum_{i=1}^{n_m}\\sum_{j:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)^2+\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n(x_k^{(i)})^2$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaborative filtering optimization objective\n",
    "1. **Given $x^{(1)},...,x^{(n_m)}$, estimate $\\theta^{(1)},...,\\theta^{(n_u)}$:**  \n",
    "$\\min\\limits_{\\theta^{(j)},...,\\theta^{(n_u)}} \\displaystyle\\frac{1}{2}\\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^T(X^{(i)})-y^{(i,j)}\\bigg)^2 + \\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^n(\\theta_k^{(j)})^2$\n",
    "<br><br>\n",
    "\n",
    "2. **Given $\\theta^{(1)},...,\\theta^{(n_m)}$, estimate $x^{(1)},...,x^{(n_u)}$:**  \n",
    "$\\min\\limits_{x^{(1)},...,x^{(n_m)}}\\displaystyle\\frac 12\\sum_{i=1}^{n_m}\\sum_{j:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)^2+\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^n(x_k^{(i)})^2$\n",
    "<br><br>  \n",
    "In both 1 and 2 $X \\text{~~} \\Large\\epsilon \\text{~~} \\mathbb{R}^{n+1} \\text{~~~}\\&\\text{~~~} \\theta \\text{~~} \\Large\\epsilon \\text{~~} \\mathbb{R}^{n+1}$ (+1 dimension is for intercept term)\n",
    "<br><br>\n",
    "3. **Minimizing $x^{(1)},...,x^{(n_m)}$ and $\\theta^{(1)},...,\\theta^{(n_u)}$ simultaeously:**  \n",
    "Minimize $\\theta \\rightarrow x \\rightarrow \\theta \\rightarrow x...$  \n",
    "$J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)}) = \\displaystyle\\frac 12 \\sum_{(i,j):r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)^2+\\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n(x_k^{(i)})^2+\\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^n(\\theta_k^{(j)})^2$\n",
    "<br><br>\n",
    "Here $X \\text{~~} \\Large\\epsilon \\text{~~} \\mathbb{R}^{n} \\text{~~~}\\&\\text{~~~} \\theta \\text{~~} \\Large\\epsilon \\text{~~} \\mathbb{R}^{n}$\n",
    "<br><br>  \n",
    "$\\min\\limits_{\\substack{x^{(1)},...,x^{(n_m)}\\\\\\theta^{(1)},...,\\theta^{(n_u)}}}J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative filtering Algorithm\n",
    "1. Initialize $x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)}$ to small random values.\n",
    "2. Minimize $J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).  \n",
    "E.g., for every $i=1,...,n_m,j=1,...,n_u$:  \n",
    "$x_k^{(i)}:= x_k^{(i)}-\\alpha\\Bigg(\\displaystyle\\sum_{j:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)\\theta_k^{(j)}+\\lambda x_k^{(i)}\\Bigg)\\text{~~~~~~~~~~~~~~~~} = x_k^{(i)}-\\alpha(\\frac{\\partial}{\\partial x_k^{(i)}}J(...))$  \n",
    "$\\theta_k^{(j)}:= \\theta_k^{(j)}-\\alpha\\Bigg(\\displaystyle\\sum_{i:r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)x_k^{(i)}+\\lambda \\theta_k^{(j)}\\Bigg)\\text{~~~~~~~~~~~~~~~~} = x_k^{(i)}-\\alpha(\\frac{\\partial}{\\partial \\theta_k^{(j)}}J(...))$  \n",
    "$\\cancel{x_0=1} \\text{~~}\\& \\text{~~} \\cancel{\\theta_0=1}$ i.e, no intercept terms needed\n",
    "<br><br>\n",
    "3. For a user with parameters $\\theta$ and a movie with (learned) features $x$, predict a star rating of $\\theta^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vecorization: Low rank matrix factorization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Movie|Alice(1)|Bob(2)|Carol(3)|Dave(4)|\n",
    "|---|---|---|---|---|\n",
    "|Love at last|5|5|0|0|\n",
    "|Romance forever|5|?|?|0|\n",
    "|Cute puppies of love|?|4|0|?|\n",
    "|Nonstop car chases|0|0|5|4|\n",
    "|Swords vs. karate|0|0|5|?|  \n",
    "\n",
    "$Y = \\begin{bmatrix} 5 & 5& 0&0\\\\\n",
    "5&?&?&0\\\\\n",
    "?&4&0&?\\\\\n",
    "0&0&5&4\\\\\n",
    "0&0&5&?\\end{bmatrix}$  \n",
    "  \n",
    "Predicted ratings:  \n",
    "$\\text{\\^{Y}} = X\\Theta^T =  \\begin{bmatrix} \n",
    "(\\theta^{(1)})^T(x^{(1)})  &  (\\theta^{(2)})^T(x^{(1)})  & \\dots   &  (\\theta^{(n_u)})^T(x^{(1)})\\\\\n",
    "(\\theta^{(1)})^T(x^{(2)})  &  (\\theta^{(2)})^T(x^{(2)})  & \\dots   &  (\\theta^{(n_u)})^T(x^{(2)})\\\\\n",
    "\\vdots                     &  \\vdots                     & \\vdots  &  \\vdots\\\\\n",
    "(\\theta^{(1)})^T(x^{(n_m)})  &  (\\theta^{(2)})^T(x^{(n_m)})  & \\dots   &  (\\theta^{(n_u)})^T(x^{(n_m)})\\\\\\end{bmatrix}$\n",
    "  \n",
    "$X = \\begin{bmatrix}\n",
    "\\text{\\textemdash}(x^{(1)})^T\\text{\\textemdash} \\\\\n",
    "\\text{\\textemdash}(x^{(2)})^T\\text{\\textemdash} \\\\\n",
    "\\vdots\\\\\n",
    "\\text{\\textemdash}(x^{(n_m)})^T\\text{\\textemdash}\n",
    "\\end{bmatrix}\n",
    "\\Theta = \\begin{bmatrix}\n",
    "\\text{\\textemdash}(\\theta^{(1)})^T\\text{\\textemdash} \\\\\n",
    "\\text{\\textemdash}(\\theta^{(2)})^T\\text{\\textemdash} \\\\\n",
    "\\vdots\\\\\n",
    "\\text{\\textemdash}(\\theta^{(n_m)})^T\\text{\\textemdash}\n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding related movies\n",
    "**For each product $i$, we learn a feature vector $x^{(i)} \\text{~~} \\Large\\epsilon \\text{~~} \\mathbb{R}^n$**  \n",
    "e.g., $x_1 = $ romance, $x_2 = $ action, $x_3 = $ comedy, $x_4 \\dots$\n",
    "<br><br>\n",
    "**How to find movies $j$ related to movie $i$?**  \n",
    "small $\\|x^{(i)} - x^{(j)}\\| \\implies j$ and $i$ are \"similar\" \n",
    "<br><br>\n",
    "**5 most similar movies to movie $i$:**  \n",
    "Find the 5 movies $j$ with the smallest $\\|x^{(i)}-x^{(j)}\\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation detail: Mean normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users who have not rated any movies\n",
    "|Movie|Alice(1)|Bob(2)|Carol(3)|Dave(4)|Eve(5)|\n",
    "|---|---|---|---|---|---|\n",
    "|Love at last|5|5|0|0|?|\n",
    "|Romance forever|5|?|?|0|?|\n",
    "|Cute puppies of love|?|4|0|?|?|\n",
    "|Nonstop car chases|0|0|5|4|?|\n",
    "|Swords vs. karate|0|0|5|0|?|  \n",
    "  \n",
    "$Y = \\begin{bmatrix} 5 & 5& 0&0&?\\\\\n",
    "5&?&?&0&?\\\\\n",
    "?&4&0&?&?\\\\\n",
    "0&0&5&4&?\\\\\n",
    "0&0&5&0&?\\end{bmatrix}$  \n",
    "\n",
    "$\\theta^{(5)}=?$\n",
    "  \n",
    "$\\min\\limits_{\\substack{x^{(1)},...,x^{(n_m)}\\\\\\theta^{(1)},...,\\theta^{(n_u)}}} \\displaystyle\\frac 12 \\sum_{(i,j):r(i,j)=1}\\bigg((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\bigg)^2+\\frac{\\lambda}{2} \\sum_{i=1}^{n_m}\\sum_{k=1}^n(x_k^{(i)})^2+\\frac{\\lambda}{2} \\sum_{j=1}^{n_u}\\sum_{k=1}^n(\\theta_k^{(j)})^2$  \n",
    "  \n",
    "**Mean Normalization**  \n",
    "$Y = \\begin{bmatrix} 5 & 5& 0&0&?\\\\\n",
    "5&?&?&0&?\\\\\n",
    "?&4&0&?&?\\\\\n",
    "0&0&5&4&?\\\\\n",
    "0&0&5&0&?\\end{bmatrix}\n",
    "\\mu=\\begin{bmatrix} 2.5\\\\2.5\\\\2\\\\2.25\\\\1.25 \\end{bmatrix}\n",
    "\\rightarrow  Y = \\begin{bmatrix} 2.5 & 2.5& -2.5&-2.5&?\\\\\n",
    "2.5&?&?&-2.5&?\\\\\n",
    "?&2&-2&?&?\\\\\n",
    "-2.25&-2.25&2.75&1.75&?\\\\\n",
    "-1.25&-1.25&3.75&-1.25&?\\end{bmatrix} \\underrightarrow{\\text{ determine }} \\text{~~~} \\theta^{(j)} \\& x^{(i)}$\n",
    "  \n",
    "For user $j$, on movie $i$ predict:  \n",
    "$\\rightarrow (\\theta^{(j)})^T(x^{(i)})+\\mu_i$\n",
    "  \n",
    "User 5(Eve):  \n",
    "$\\theta^{(5)} = \\begin{bmatrix}0\\\\0\\end{bmatrix} \\text{~~~~~~~~~} (\\theta^{(5)})^T(x^{(i)})+\\mu_i$  \n",
    "which will give the mean of old users as a new rating of new user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Scale machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning with Large datasets\n",
    "> It's not who has the best algorithm that wins, It's who has the most data  \n",
    "\n",
    "If m = 100,000,000 then try to verify if the algorithm has high varience for small set of data like m=1000, if it has high bias with small $m$ then stop the training with larger $m$ as cost $J$ highly likely to be remain constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "It will be able to handle the training with huge data  \n",
    "<br><br>\n",
    "**Linear regression with gradient descent**  \n",
    "$h_{\\theta}(x) = \\displaystyle\\sum_{j=0}^n\\theta_jx_j$  \n",
    "$J_{train}(\\theta) = \\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$  \n",
    "<br>\n",
    "Repeat $\\bigg\\{$  \n",
    "$\\theta_j := \\theta_j -\\alpha\\displaystyle\\frac{1}{m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})x_j^{(i)}$  \n",
    "(for every $j=0,...,n$)  \n",
    "$\\bigg\\}$  \n",
    "Here for each and every update of $\\theta_j$ we need to go over m data samples if the m>100,000,000 then this each tiny steps will cost very high   \n",
    "this process is also known as **Batch gradient descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Batch gradient descent|Stochastic gradient descent|\n",
    "|---|---|\n",
    "|$J_{train}(\\theta) = \\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$|$cost(\\theta,(x^{(i)},y^{(i)})) = \\displaystyle\\frac 12 (h_{\\theta}(x^{(i)}-y^{(i)})^2 \\\\ J_{train}(\\theta) = \\frac 1m \\sum_{i=1}^mcost(\\theta, (x^{(i)},y^{(i)}))$|\n",
    "|$\\text{Repeat} \\bigg\\{ \\\\ \\theta_j := \\theta_j -\\alpha\\displaystyle\\frac{1}{m}\\sum_{i=1}^m\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_j^{(i)}$<br>(for every $j=0,...,n$)<br>$\\bigg\\}$<br>entire dataset will be considered for each gradient calculation|1. Randomly shuffle dataset<br>2. Repeat $\\bigg\\{ $<br>$ \\text{~~~~~~~~for~~} i=1,...,m \\{ $<br>$ \\text{~~~~~~~~~~~~} \\theta_j:=\\theta_j-\\alpha\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_j^{(i)} \\\\ \\text{~~~~~~~~~~~~~~~~(for } j=0,...,n) \\\\ \\text{~~~~~~~~~~~~}\\} \\\\ \\text{~~~~~~~~}\\bigg\\}$<br>$\\therefore$ for each new data point in dataset the gradient will be calculated|  \n",
    "  \n",
    "> NOTE:  Stochastic gradient will not converge to a single fixed value like Batch gradient descent but rather it will be oscillating around glboal minimum.<br> this is due to gradients are calculated at each new datapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batch gradient descent\n",
    "**Batch gradient descent:** Use all $m$ examples in each iteration  \n",
    "**Stochastic gradient descent:** Use only 1 example in each iteration  \n",
    "**Mini-batch gradient descent:** Uset $b$ examples in each iteration  \n",
    "$b = 2$ to $100$  \n",
    "let $b=10, m=1000$  \n",
    "Repeat $\\bigg\\{ \\\\ \\text{~~~~~~~~~~~~~~~~for~}i=1,11,21,31,...,991 \\{ $<br>\n",
    "$\\displaystyle\\text{~~~~~~~~~~~~~~~~~~~~~~~~}\\theta_j:=\\theta_j-\\alpha\\frac{1}{10}\\sum_{k=i}^{i+9}(h_{\\theta}(x^{(k)})-y^{(k)})x_j^{(k)}$<br>\n",
    "$\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~(for every }j=0,...n)$<br>\n",
    "$\\text{~~~~~~~~~~~~~~~~~~~~}\\}$<br>\n",
    "$\\text{~~~~~~~~~~~~}\\bigg\\}$\n",
    "<br><br>\n",
    "**advantages** over stochastic gradient descent is $vectorization$,  \n",
    "i.e., able to perform vector computation techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent convergence\n",
    "**Checking for convergence**  \n",
    "Batch gradient descent:<br>  \n",
    "$\\text{~~~~}$ Plot $J_{train}(\\theta)$ as a function of the number of iterations of gradient descent  \n",
    "$\\text{~~~~}$  $J_{train}(\\theta)=\\displaystyle\\frac{1}{2m}\\sum_{i=1}^m(h_{\\theta}(x^{(i)})-y^{(i)})^2$\n",
    "<br><br>\n",
    "Stochastic gradient descent:<br>\n",
    "$\\text{~~~~}$ $cost(\\theta,(x^{(i)},y^{(i)})) = \\displaystyle\\frac 12 (h_{\\theta}(x^{(i)})-y^{(i)})^2$  \n",
    "$\\text{~~~~}$ During learning, compute $cost(\\theta,(x^{(i)},y^{(i)}))$ before updating $\\theta$ using $(x^{(i)},y^{(i)})$.<br>  \n",
    "$\\text{~~~~}$ Every 1000 iterations (say), plot $cost(\\theta,(x^{(i)},y^{(i)}))$ averaged over the last 1000 examples processed by algorithm.\n",
    "> Learning rage $\\alpha$ is typically held constant. but can slowly decrease $\\alpha$ over time if we want $\\theta$ to converge. (E.g. $\\alpha = \\displaystyle\\frac{const1}{iterationNumber+const2}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\Stochastic grad convergence.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\Stochastic gradient eg.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large scale machine learning\n",
    "## Online learning  \n",
    "Shipping service website where user comes, specifies  origin and destination, you offer to ship their package for some asking price, and user sometimes choose to use your shipping service $(y=1)$, sometimes not $(y=0)$.  \n",
    "Features $x$ capture properties of user, of origin/destination and asking price. We want to learn $p(y=1|x;\\theta)$ to optimize price  \n",
    "Repeat forever $\\{$  \n",
    "$\\text{~~~~~~~~~~~~~Get } (x,y) \\text{ corresponding to user.}$  \n",
    "$\\text{~~~~~~~~~~~~~Update } \\theta \\text{ using } (x,y): \\xcancel{(x^{(i)},y^{(i)})}$  \n",
    "$\\text{~~~~~~~~~~~~~~~~~} \\theta_j := \\theta_j - \\alpha(h_{\\theta}(x)-y).x_j \\text{~~~~~~~~}(j=0,...,n)$  \n",
    "$\\text{~~~~~~~~~~~~~}\\}$  \n",
    "<br>  \n",
    "> Can adapt to changing user preference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other online learning example:\n",
    "Product search(learning to search)  \n",
    "$\\text{~~~~}$ User Searches for \"$\\text{Android phone 1080p camera}$\"  \n",
    "$\\text{~~~~}$ Have 100 phones in store. Will return 10 results.  \n",
    "$\\text{~~~~~} x=$ features of phone, how many words in user query match name of phone,  \n",
    "$\\text{~~~~~~~~~~~~}$ how many words in query match description of phone, etc.  \n",
    "$\\text{~~~~~} y=1 $ if user clicks on link. $y=0$ otherwise.  \n",
    "$\\text{~~~~}$ Learn $p(y=1|x;\\theta)$.  \n",
    "$\\text{~~~~}$ Use to show the 10 phones they're most likely to click on.\n",
    "<br>  \n",
    "**Other examples**:  \n",
    "Choosing special offers to show user;  \n",
    "customized selection of news articles;  \n",
    "product recommendation;.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map-reduce and data parallelism\n",
    "### Map-reduce\n",
    "Batch gradient descent  \n",
    "let m=400  \n",
    "now instead of calculating the $\\sum_{i=1}^{400}(h_{\\theta}(x)-y)^2$ on one machine we can distribute the load over 4 machines (say)   \n",
    "then each machine will compute the 1/4 th of the total data i.e.,<br>   \n",
    "$\\sum_{i=1}^{100}(h_{\\theta}(x)-y)^2$<br>   \n",
    "$\\sum_{i=101}^{200}(h_{\\theta}(x)-y)^2$<br>  \n",
    "$\\sum_{i=201}^{300}(h_{\\theta}(x)-y)^2$<br>  \n",
    "$\\sum_{i=301}^{400}(h_{\\theta}(x)-y)^2$<br>  \n",
    "  \n",
    "and finally combining this sum in a central machine and compute $\\sum_{i=1}^{400}(h_{\\theta}(x)-y)^2$  \n",
    "Which means the batch gradient descent is computed $\\leq$ 4x faster<br>  \n",
    "\n",
    "The same can be achieved over a 4 different machines or a 4-core processor  \n",
    "> 4-core processor performs better than using 4 different machines as the latency will be minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application example: Photo OCR\n",
    "## Problem description and pipeline  \n",
    "1. Text detection\n",
    "2. Character segmentation\n",
    "3. Character classification  \n",
    "\n",
    "Image $\\rightarrow$ Text detection $\\rightarrow$ Character segmentation $\\rightarrow$ Character recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting lots of data: Artificial data synthesis\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ceiling analysis: what part of the pipeline to work on next\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you perform ceiling analysis on a pipelined  machine learning system, and when we plug in the ground-truth labels for one of the components, the performance of the overall system improves very little. This probably means:\n",
    "1. It is probably not worth dedicating engineering resources to improve that component of the system.\n",
    "2. If that component is a classifier training using gradient descent, it is probably not worth running gradient descent for 10x as long to see if it converges to better classifier parameters."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
