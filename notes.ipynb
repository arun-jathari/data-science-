{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial MSE}{\\partial \\theta_0} = -\\frac{2}{m}\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)$$\n",
    "$$\\frac{\\partial MSE}{\\partial \\theta_1} = -\\frac{2}{m}\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)\\big(x^{(i)}\\big)$$  \n",
    "\n",
    "Here $\\theta_0$ is intercept and $\\theta_1$ is slope\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **model** |  |\n",
    "| --- | ---|\n",
    "| init |initialize the coefficiesnts like intercept and slope|\n",
    "| predict |estimate the output based on the current coefficiesnts values|\n",
    "| get summary |the user friendly output summary|\n",
    "|  |  |\n",
    "|**cost function** | it calucaltes the difference between the true and predicated values <br> eg.: MSE, MAE |\n",
    "|  |  |\n",
    "| **optimizer** | |\n",
    "| calculate gradients | it will calculate the gradients of the errors and find out the new deltas of variables <br> eg.: $\\partial$ slope & $\\partial$ intercept values |\n",
    "| perform gradient descent | here the new values of variables will be caluclated|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "<i>hyper params</i>: learning rate $\\eta$, no. of iterations   \n",
    "  \n",
    "new $\\theta_1$ = old $\\theta_1 - \\eta * \\frac{\\partial MSE}{\\partial \\theta_1} $  \n",
    "  \n",
    "new $\\theta_0$ = old $\\theta_0 - \\eta * \\frac{\\partial MSE}{\\partial \\theta_0} $ \n",
    "\n",
    "note: $\\eta$ will decide the stepsize of descent.  \n",
    "&emsp;&emsp;larger $\\eta$ may overshoot the local minima.  \n",
    "&emsp;&emsp;smaller $\\eta$ will take longer time and may not reach the minima\n",
    "\n",
    "**step 1**  \n",
    "calculate the difference of the true and predicted values i.e., $\\displaystyle\\sum_{i=1}^{m}\\big(y^{(i)}-\\theta_0-\\theta_1 x^{(i)}\\big)$\n",
    "   \n",
    "**step 2**  \n",
    "calculate the partial derivatives i.e., $\\frac{\\partial MSE}{\\partial \\theta_1}, \\frac{\\partial MSE}{\\partial \\theta_0}$  \n",
    "  \n",
    "**step 3**  \n",
    "calculate the new $\\theta_0 \\& \\theta_1$ i.e., model's intercept and slope\n",
    "\n",
    "**step 4**  \n",
    "continue the steps from 1 to 3 untill the cost becomes negligible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UnSupervised Learning\n",
    "<img src=\"media/UnSupervised Learning.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Learning\n",
    "<img src=\"media/Supervised Learning.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function (Objective Function)\n",
    "It helps in maintaining consistant optimization of the model. if cost function always shows smaller errors then optimizer will make smaller changes in the model parameters, if cost function shows larger error for some updates then the optimizer will maker sure that these kind of changes are avoided  \n",
    "e.g.: MSE will throw large error for outliers compared to MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "<img src=\"media/Gradient Descent Graphs.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good and Bad of Data  \n",
    "Is Large amount of data is good?\n",
    "* yes, if it is representative\n",
    "* Doesn't contain errors\n",
    "* Aren't missing key information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Representative?  \n",
    "**Sample Data**: it is a subset of the total data which is good enough to express the whole data  \n",
    "&emsp; e.g.: Like Survays made of discrete people and make inferences on all people  \n",
    "**Population Data**: It is every data available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Errors in data?  \n",
    "**Skewed Data**: if majority data collected is favoring one set of outputs then it is called skewed data  \n",
    "&emsp; e.g.: student data contains $90\\%$ girls and $10\\%$ boys  \n",
    "**Measurement error**: Errors due to the Insturment/tool used to collect the data  \n",
    "&emsp; e.g.: rouding errors in data where scale is Billion $\\$$  \n",
    "**Data Entry errors**: Incorrect entry of data at time of data collection  \n",
    "&emsp; e.g.: size of the height of human is recorded as 18mts instead of 1.8mts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Missing Data](https://towardsdatascience.com/all-about-missing-data-handling-b94b8b5d2184)  \n",
    "If there is any missing information in any of the column of the collected data then it is called as Missing Data  \n",
    "which can be imputed by many procedures like mean, mode, regression etc,.  \n",
    "or  \n",
    "remove the samples which are incomplete  \n",
    "or  \n",
    "Use a model which can accept these missing data e.g.: k-Nearest Neighbours, classification and regression trees, XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data  \n",
    "### Continuous, ordinal, and categorical data  \n",
    "Continuous data: it is numeric data to which value can be added and deleted  \n",
    "&emsp; e.g.: age, height, income etc,.  \n",
    "  \n",
    "Categorical data: Data containing Labels or categories  \n",
    "&emsp; e.g.: gender, occupation, genre etc,.  \n",
    "  \n",
    "Ordinal data: data which have highrarchical order which can be expressed as sequence of numbers  \n",
    "&emsp; e.g.: rank, ratings, income level etc,."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datatypes  \n",
    "### Primitive  \n",
    "**integers:** counting numbers, like 2  \n",
    "**floating-point numbers:** numbers with decimal places, like 2.43  \n",
    "**strings:** letters and words  \n",
    "**booleans:** true and false  \n",
    "**None, void, or null:** not data, but rather the absence of it  \n",
    "  \n",
    "### Derivative  \n",
    "Its a combination of one or more primitive datatypes  \n",
    "e.g.: images, dates, 3d models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to handle these different types of data?  \n",
    "for continuous data, floating point numbers are best  \n",
    "for ordinal data, encode them into integers  \n",
    "for categorical data,  \n",
    "&emsp; if the data contains only two categories, then use either boolean or binary encoding  \n",
    "&emsp; if it has more than two categories, use dummy variables/one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding, data cleaning, and statistical power  \n",
    "before performing we need to know whether what kind of impact it gonna make it can found out by statistical power  \n",
    "Statistical Power is the ability of the model to correctly identify the relations between features and labels  \n",
    "  \n",
    "**removing data:** removing data can lower the statistical power  \n",
    "&emsp; e.g.: skewed data like 99 girls 1 boy  \n",
    "**Unnessary columns:** unnessary columns can reduce the statistical power  \n",
    "&emsp; e.g.: gender information in a performance measurement  \n",
    "**one hot encoding** can also lowers the Stastical power much more than the continuous or categorical data  \n",
    "&emsp; e.g.: column with 200 categories in 1000 row data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy  \n",
    "Its a python library which will provide the functionality to perform comparable mathematical operations like MATLAB and R. It helps user building complex mathematical functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas  \n",
    "It is also a python library which helps user to understand the data in tabular formats, in short it is kind of the python version of excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing  \n",
    "Data analytics and visualizations are iterative processess typically  \n",
    "**Data cleaning** and **imputing Missing values** in data  \n",
    "**Applying Statistical techniques** to understand if the sample data abel to represent the whole data  \n",
    "**Data Visualization** are used to understand the realations between various columns  \n",
    "**Hypothesis** is revised and repeat the above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers  \n",
    "the data points which are beyound the normal data/reasonable data is known as outliers  \n",
    "some of these outliers can be found easily and for others we need to have the understanding of the data theoritically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias & Variance  \n",
    "These are variations in the data which can significantly effect our model performance, need to take care these data points through central tendency concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression  \n",
    "Advantages:  \n",
    "Predictable and easy to interpret  \n",
    "easy to extrapolate  \n",
    "Optimal solution is usually garanteed\n",
    " - It can find the Optimal solution with garantee if the dataset is small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Leanear Regression Assumptions  \n",
    "Every feature considered are assumed to be linearly independent, if not then the result of model can be misleading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Goodness of fit: $R^2$ (Coefficient of Determination)  \n",
    "is the correlation between x and y squared.  \n",
    "Strong correlations means larger $R^2$  \n",
    "$R^2=1$ : pefectly predicts the output  \n",
    "$R^2=0$ : no relation between the variables  \n",
    "  \n",
    "$R^2$ limitations:\n",
    "* Model with high data will have high $R^2$ compared to same model with low data  \n",
    "* Cannot garantees the models prediction power on unseen data  \n",
    "* It cannot tell the direction of relation between variables\n",
    "  \n",
    "In complex models $R^2=0.3$ can be a perfect score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Models  \n",
    "### Linear Regression  \n",
    "- it is the simplest of all the regression models  \n",
    "- no limit on the no of features used  \n",
    "- it comes in many forms\n",
    "    - Simple Linear Regression\n",
    "    - Multiple Linear Regression\n",
    "    - Polynomial Regression etc,.\n",
    "- e.g.: ols, Lasso and Ridge regression\n",
    "### Decision Trees  \n",
    "- A step-by-step approch\n",
    "- it splits the data into based on the higher entropy\n",
    "### Ensemble algorithms  \n",
    "- it is a group of multiple trees\n",
    "    - e.g.: Random Forest\n",
    "- its prediction capabilities are strong\n",
    "- it imporves the generality of model\n",
    ">  [Scikit-Learn documentation](https://scikit-learn.org/stable/supervised_learning.html)<br>[Scikit-Learn estimator cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "$$\\boxed{J(\\theta) = \\displaystyle\\frac{1}{2m}\\displaystyle\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^{2}}$$\n",
    "  \n",
    "$h_{\\theta}(x) = \\theta^{T}x = \\theta_{0} + \\theta_{1}*x$  \n",
    "in terms of matrix multiplication we can do it as  \n",
    "$\\implies \\boxed{h(\\theta) = \\begin{bmatrix} 1 & x_{0} \\\\ 1 & x_{1} \\\\ 1 & x_{2} \\end{bmatrix} \\times \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\end{bmatrix}}$  \n",
    "i.e., $h(\\theta) = DataMatrix \\times Parameters$  \n",
    "  \n",
    "Similarly we can compare the multilple combinatios of these parameters as  \n",
    "$\\begin{bmatrix} 1 & x_{0} \\\\ 1 & x_{1} \\\\ 1 & x_{2} \\end{bmatrix} \\times \\begin{bmatrix} \\theta_{0}^{1} & \\theta_{0}^{2} & \\theta_{0}^{2} \\\\ \\theta_{1}^{1} & \\theta_{1}^{2} & \\theta_{1}^{3} \\end{bmatrix} = \\begin{bmatrix} h^{1}(\\theta) & h^{2}(\\theta) & h^{3}(\\theta) \\end{bmatrix}_{3 \\times 3}$  \n",
    "  \n",
    "**Properties of Matrix Multiplication (In general):**  \n",
    "A x B $\\ne$ B x A (not commutative)  \n",
    "A x (B x C) = (A x B) x C (Associative)  \n",
    "$I_{m \\times m}$ x $A_{m \\times n}$ = $A_{m \\times n}$ x $I_{n \\times n}$ = $A_{m \\times n}$ (Identity matrix)  \n",
    "If A is an (m x m) matrix and if it has an inverse  \n",
    "$A \\times A^{-1}=A^{-1} \\times A=I_{m \\times m}$  \n",
    "&emsp;*Note:* Matrices that don't have an inverse are called as \"singular\" or \"degenerate\"  \n",
    "$A^{T} = B$ (Matrix Transpose)  \n",
    "$\\implies B_{ij} = A_{ji}$\n",
    "  \n",
    "  \n",
    "For Multi variable Linear regression\n",
    "$X = \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}_{n+1,1} (where \\space x_{0}=1) ,\\space \\space \\space\n",
    "\\Theta = \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\vdots \\\\ \\theta_{n} \\end{bmatrix}_{n+1, 1}$\n",
    "$\\because \\space h_{\\theta}(x) = \\theta_{0} + \\theta_{1}*x_{1} + \\theta_{2}*x_{2} + \\dots + \\theta_{n}*x_{n}$  \n",
    "  \n",
    "$\\implies h_{\\theta}(x)= \\begin{bmatrix} \\theta_{0} & \\theta_{1} & \\dots & \\theta_{n} \\end{bmatrix} \\times$\n",
    "$\\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\vdots \\\\ x_{n} \\end{bmatrix}$\n",
    "$=  \\Theta^{T} \\times X$  \n",
    "  \n",
    "here n is number of features  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "new algorithm $(n \\ge 1)$  \n",
    "  \n",
    "Repeat $\\bigg\\{$  \n",
    "$$\\boxed{\\theta_{j} = \\theta_{j} - \\alpha \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}$$ \n",
    "(simultaneously update $\\theta_{j}$ for j = 0, $\\dots$, n)  \n",
    "$\\bigg\\}$  \n",
    "where $x_{0}^{(i)}=1$  for (i $\\epsilon$ 1, $\\dots$, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Scaling**  \n",
    "* Mean Normalization\n",
    "$$x_{i}:=\\frac{x_{i}-\\mu_{i}}{s_{i}}$$\n",
    "$s_{i} = max(x_{i}) - min(x_{i})$  \n",
    "standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation  \n",
    "  \n",
    "$\\theta = (X^{T}X)^{-1}X^{T}y$\n",
    "  \n",
    "here X is of (m x (n+1)) dimension, where first column will be all 1's i.e., $x_{0}^{i}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|**Gradient Descent** | **Normal Equation**|  \n",
    "| --- | ---|\n",
    "| Need to choose alpha | No need to choose alpha|  \n",
    "| Needs many iterations | No need to iterate|  \n",
    "|$O(kn^{2})$ | $O(n^{3})$, need to calculate inverse of $X^{T}X$|  \n",
    "| Works well when n is large | Slow if n is very large|\n",
    "|consider $m>10^{6}$| $m<10^{6}$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if $X^{T}X$ is non invertable  \n",
    "* Redundant features(Linearly dependent)  \n",
    "    e.g.: $x_{1} = 2x_{2}$\n",
    "* Too many features (e.g: $m \\le n$)\n",
    "    * delete some features or use regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cost function in Multivariate case using vectors Gradient Descent \n",
    "$$\\boxed{J(\\theta) = \\frac{1}{2m}(X\\Theta - y)^{T}(X\\Theta - y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**\n",
    "$$\\boxed{\\Theta = (X^{T}X)^{-1}X^{T}y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression  \n",
    "  \n",
    "$h_{\\theta}(x) = g(\\theta^{T}x)$  \n",
    "  \n",
    "$g(z) = \\displaystyle\\frac{1}{1+e^{-z}}$  \n",
    "  \n",
    "$\\boxed{\\therefore h_{\\theta}(x) = \\displaystyle\\frac{1}{1+e^{-\\theta^{T}x}}}$  \n",
    "  \n",
    "$\\implies 0\\le h_{\\theta}(x)\\le 1$  \n",
    "\n",
    "$\\boxed{h_{\\theta}(x) = P(y=1|_{x};\\theta)}$  \n",
    "\"porbability that y=1, given x, parameterized by $\\theta$\"  \n",
    "  \n",
    "$\\boxed{P(y=0|_{x};\\theta) = 1 - P(y=1|_{x};\\theta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Boundary**  \n",
    "  \n",
    "$h_{\\theta} \\ge 0.5 \\rightarrow y = 1$  \n",
    "$h_{\\theta} < 0.5 \\rightarrow y = 0$  \n",
    "  \n",
    "$\\because g(z) \\ge 0.5$  \n",
    "when $z\\ge 0$  \n",
    "\n",
    "$*$ Note:  \n",
    "$z = 0, e^{0} = 1 \\implies g(z) = 1/2$  \n",
    "$z \\rightarrow \\infin, e^{-\\infin} \\rightarrow 0 \\implies g(z) = 1$  \n",
    "$z \\rightarrow -\\infin, e^{\\infin} \\rightarrow \\infin \\implies g(z) = 0$  \n",
    "  \n",
    "$\\boxed{\\begin{matrix}\\theta^{T}x \\ge 0 \\implies y = 1\\\\\n",
    "\\theta^{T}x < 0 \\implies y = 0\\end{matrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "due to $g(z)$ in Logistic Regression $\\implies$ non-convex cost function  \n",
    "$\\therefore$ consider following cost function which will result in convex cost function  \n",
    "  \n",
    "$$J(\\theta) = \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}Cost(h_{\\theta}(x^{(i)}),y^{(i)})$$    \n",
    "\n",
    "$Cost(h_{\\theta}(x),y) =\\begin{cases} \n",
    "    -log(h_{\\theta}(x)) & \\text{if~~} y=1 \\\\ \n",
    "    -log(1-h_{\\theta}(x)) & \\text{if~~}y=0 \n",
    "\\end{cases}$  \n",
    "  \n",
    "$$\\implies Cost(h_{\\theta}(x),y) = -y\\text{~}log(h_{\\theta}(x))-(1-y)log(1-h_{\\theta}(x))$$  \n",
    "  \n",
    "$$\\implies \\boxed{J(\\theta) = - \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg[y^{(i)}log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))\\bigg]}$$  \n",
    "  \n",
    "Vector Form  \n",
    "$\\boxed{\\begin{matrix}J(\\theta) = \\displaystyle\\frac{1}{m}\\bigg[-y^{T}log(h)-(1-y)^{T}log(1-h)\\bigg] \\\\\n",
    "  \\\\  \n",
    "\\text{where } h = g(\\theta^{T}X)\\end{matrix}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\Linear Regression.drawio.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Cost(h_{\\theta}(x),y) = 0 \\text{~~if~~} h_{\\theta}(x)=y$  \n",
    "$Cost(h_{\\theta}(x),y) \\rightarrow \\infin \\text{~~if~~}y=0 \\text{~~and~~} h_{\\theta}(x) \\rightarrow 1$  \n",
    "$Cost(h_{\\theta}(x),y) \\rightarrow \\infin \\text{~~if~~}y=1 \\text{~~and~~} h_{\\theta}(x) \\rightarrow 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "$J(\\theta) = - \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg[y^{(i)}log\\text{~}h_{\\theta}(x^{(i)}) + (1-y^{(i)})log\\big(1-h_{\\theta}(x^{(i)})\\big)\\bigg]$  \n",
    "  \n",
    "Want $ \\min\\limits_{\\theta}J(\\theta):$\n",
    "  \n",
    "$Repeat \\bigg\\{    \n",
    "\\theta_{j}:=\\theta_{j}-\\alpha \\frac{\\partial}{\\partial \\theta_{j}}J(\\theta)  \n",
    "\\bigg\\} $\n",
    "  \n",
    "(simultaniously update all $\\theta_{j}$)  \n",
    "  \n",
    "where $\\displaystyle\\frac{\\partial}{\\partial \\theta_{j}} J(\\theta) = \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)}$\n",
    "\n",
    "$\\boxed{\\therefore Repeat \\bigg\\{    \n",
    "\\theta_{j}:=\\theta_{j}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)}  \n",
    "\\bigg\\}} $  \n",
    "  \n",
    "where $h_{\\theta}(x) = \\displaystyle\\frac{1}{1+e^{-\\theta^{T}x}}$   \n",
    "  \n",
    "vector form  \n",
    "$$\\boxed{\\theta = \\theta - \\frac{\\alpha}{m}X^{T}\\bigg(g(X^{T}\\theta)-\\overrightarrow{y}\\bigg)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization algorithms**\n",
    "* Gradient descent\n",
    "* Conjugate gradient\n",
    "* BFGS\n",
    "* L-BFGS\n",
    "  \n",
    "Advanced algorithms  \n",
    "* Advantages:\n",
    "    * No need to manually pick $\\alpha$\n",
    "    * Often faster than gradient descent  \n",
    "  \n",
    "* Disadvantages:\n",
    "    * More complex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Logistic Regression\n",
    "<img src=\"media\\one-vs-all.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\implies h_{\\theta}^{(i)}(x) = P(y=i|_{x};\\theta)\\text{~~~~~~~~~}(i=1,2,3)$  \n",
    "\"porbability that $y=i$ (class), given $x$, parameterized by $\\theta$\"  \n",
    "  \n",
    "Train a logistic regression classifier $h_{\\theta}^{(i)}(x)$ for each class $i$ to predict the probability that $y=i$.  \n",
    "  \n",
    "On a new input $x$, to make a prediction, pick the class $i$ that maximizes  \n",
    "$\\implies \\boxed{prediction = \\operatorname*{max}\\limits_i h_{\\theta}^{(i)}(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "**Over Fitting**  \n",
    "\n",
    "Types of fitting:  \n",
    "* Under fitting or High Bias\n",
    "* right fit\n",
    "* Over fitting or High Varience\n",
    "\n",
    "How to address Overfitting  \n",
    "1) Reduce the number of features:\n",
    "\n",
    "    * Manually select which features to keep.\n",
    "\n",
    "    * Use a model selection algorithm (studied later in the course).\n",
    "\n",
    "2) Regularization\n",
    "\n",
    "    * Keep all the features, but reduce the magnitude of parameters $\\theta_{j}$  \n",
    "  \n",
    "    * Regularization works well when we have a lot of slightly useful features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boxed{\\begin{matrix} \\therefore J(\\theta) = \\displaystyle\\frac{1}{2m}\\bigg[\\overbrace{\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)^{2}}^{\\text{cost}}+\\overbrace{\\lambda \\sum_{j=1}^{n}\\theta_{j}^{2}\\bigg]}^{\\text{Regularization Term}} \\\\\n",
    "\\\\\n",
    "\\text{i.e., } \\min\\limits_\\theta J(\\theta) \\text{~~~~~~~(minimizing the cost by minimizing }\\theta) \\\\\n",
    "\\\\\n",
    "\\lambda \\text{ : regularization parameter}\\end{matrix}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\regularization.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Descent**  \n",
    "Repeat $\\bigg\\{$  \n",
    "$\\theta_{0}:=\\theta_{0}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{0}^{(i)}$  \n",
    "  \n",
    "$\\theta_{j}:=\\theta_{j}-\\alpha \\bigg[\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg] \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n)$  \n",
    "$\\bigg\\}$\n",
    "  \n",
    "the second eq'n also writen as  \n",
    "$\\implies \\theta_{j}:=\\theta_{j}\\underbrace{\\bigg(1-\\alpha\\displaystyle\\frac{\\lambda}{m}\\bigg)}_{<1}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} \n",
    " \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Equation**  \n",
    "$\\boxed{\\theta = \\Biggr(X^{T}X + \\lambda\\begin{bmatrix} 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & 1 \\end{bmatrix}_{n+1,n+1}\\Biggr)^{-1}X^{T}y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non invertable**   \n",
    "if $m\\le n$    \n",
    "$\\implies \\theta \\neq (\\underbrace{X^{T}X}_{\\text{non-invertable}})^{-1}X^{T}y$  \n",
    "  \n",
    "$\\because X^T X$ is non invertable  \n",
    "  \n",
    "But if $\\lambda > 0$  \n",
    "$\\theta = \\Biggr(\\underbrace{X^{T}X + \\lambda\\begin{bmatrix} 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & \\ddots & \\vdots \\\\\n",
    "\\vdots & \\ddots & \\ddots & 0 \\\\\n",
    "0 & \\dots & 0 & 1 \\end{bmatrix}_{n+1,n+1}}_{\\text{invertable}}\\Biggr)^{-1}X^{T}y$  \n",
    "i.e., regularization overcomes the singularity problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression with regularization  \n",
    "  \n",
    "$J(\\theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^n\\theta_j^2$  \n",
    "\n",
    "**Gradient Descent**: Logistic Regression gradient descent is same as Linear Regression  \n",
    "Repeat $\\bigg\\{$  \n",
    "$\\theta_{0}:=\\theta_{0}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{0}^{(i)}$  \n",
    "  \n",
    "$\\theta_{j}:=\\theta_{j}-\\alpha \\bigg[\\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} + \\frac{\\lambda}{m}\\theta_{j}\\bigg] \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n)$  \n",
    "$\\bigg\\}$\n",
    "  \n",
    "the second eq'n also writen as  \n",
    "$\\implies \\theta_{j}:=\\theta_{j}\\underbrace{\\bigg(1-\\alpha\\displaystyle\\frac{\\lambda}{m}\\bigg)}_{<1}-\\alpha \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(h_{\\theta}(x^{(i)})-y^{(i)}\\bigg)x_{j}^{(i)} \n",
    " \\text{~~~~~~~~~~~~~~}(j = 1,2,3,...,n) $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network\n",
    "<img src=\"media\\neural network.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in simpler terms  \n",
    "$[x_0x_1x_2x_3] \\rightarrow [a_1^{(2)}a_2^{(2)}a_3^{(2)}] \\rightarrow h_{\\theta}(x)$  \n",
    "  \n",
    "$a_i^{(j)} = $\"activation\" of unit $i$ in layer $j$  \n",
    "$\\Theta^{(j)} = $ matrix of weights controlling function mapping  from layer $j$ to layer $j+1$  \n",
    "  \n",
    "$a_1^{(2)} = g(\\Theta_{10}^{(1)}x_0 + \\Theta_{11}^{(1)}x_1 + \\Theta_{12}^{(1)}x_2 + \\Theta_{13}^{(1)}x_3)$  \n",
    "  \n",
    "$a_2^{(2)} = g(\\Theta_{20}^{(1)}x_0 + \\Theta_{21}^{(1)}x_1 + \\Theta_{22}^{(1)}x_2 + \\Theta_{23}^{(1)}x_3)$  \n",
    "  \n",
    "$a_3^{(2)} = g(\\Theta_{30}^{(1)}x_0 + \\Theta_{31}^{(1)}x_1 + \\Theta_{32}^{(1)}x_2 + \\Theta_{33}^{(1)}x_3)$  \n",
    "  \n",
    "$h_{\\Theta}(x) = a_1^{(3)} = g(\\Theta^{(2)}_{10}a_{0}^{(2)} + \\Theta^{(2)}_{11}a_{1}^{(2)} + \\Theta^{(2)}_{12}a_{2}^{(2)} + \\Theta^{(2)}_{13}a_{3}^{(2)})$  \n",
    "  \n",
    "If network has $s_j$ units in layer $j$, $s_{j+1}$ units in layer $j+1$, then $\\Theta^{(j)}$ will be of dimension $s_{j+1} \\times (s_j + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let  \n",
    "  \n",
    "$a_1^{(2)} = g(z_1^{(2)})$  \n",
    "  \n",
    "$a_2^{(2)} = g(z_2^{(2)})$  \n",
    "  \n",
    "$a_3^{(2)} = g(z_3^{(2)})$  \n",
    "  \n",
    "i.e., $z_k^{(2)} = \\Theta_{k,0}^{(1)}x_0 + \\Theta_{k,1}^{(1)}x_1 + \\dots + \\theta_{k,n}^{(1)}x_n$  \n",
    "  \n",
    "in vector form  \n",
    "$x = \\begin{bmatrix} x_0 \\\\ x_1 \\\\ \\dots \\\\ x_n \\end{bmatrix}$    $z^{(j)} = \\begin{bmatrix} z_0^{(j)} \\\\ z_1^{(j)} \\\\ \\dots \\\\ z_n^{(j)} \\end{bmatrix}$    \n",
    "  \n",
    "$\\boxed{\\therefore a^{(j)} = g(z^{(j)}) \\impliedby  z^{(j)} = \\Theta^{(j-1)}a^{(j-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**E.g., Build XNOR Gate**  \n",
    "the $\\Theta^{(1)}$ matrices for AND, NOR and OR are:  \n",
    "$AND: \\Theta^{(1)} = \\begin{bmatrix} -30 \\\\ 20 \\\\ 20 \\end{bmatrix} \\text{,~~~ }\n",
    "NOR: \\Theta^{(1)} = \\begin{bmatrix} 10 \\\\ -20 \\\\ -20 \\end{bmatrix} \\text{,~~~~ }\n",
    "OR: \\Theta^{(1)} = \\begin{bmatrix} -10 \\\\ 20 \\\\ 20 \\end{bmatrix}$  \n",
    "$\\text{\\\\ \\\\}$\n",
    "$$\\implies \\underbrace{\\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\end{bmatrix}}_{L1} \\xRightarrow{\\Theta^{(1)}} \\underbrace{\\begin{bmatrix} a_1^{(2)} \\\\ a_2^{(2)} \\end{bmatrix}}_{L2} \\xRightarrow{\\Theta^{(2)}} \\underbrace{\\begin{bmatrix} a^{(3)} \\end{bmatrix}}_{L3} \\rightarrow h_{\\Theta}(x)$$  \n",
    "$\\text{\\\\ \\\\}$  \n",
    "$\\text{where ~~~~~~} \\Theta^{(1)} = \\begin{bmatrix} -30 & 10 \\\\ 20 & -20 \\\\ 20 & -20 \\end{bmatrix} \n",
    "\\text{~~~~~~~~~~ \\& ~~~~~~~~~} \n",
    "\\Theta^{(2)} = \\begin{bmatrix} -10 \\\\ 20 \\\\ 20 \\end{bmatrix}$ \n",
    "$\\Downarrow$\n",
    "|| dimensions|\n",
    "|--|--|\n",
    "|$a^{(2)} = g(\\Theta^{(1)}.x)$ | $(2\\times 1) = (3\\times 2)^T*(3\\times 1)$|\\\\\n",
    "|$$\\downarrow$$| bias term $a_0^{(2)} = 1$ is added to $a^{(2)}$ i.e.,$ (2\\times 1)\\rightarrow (3\\times 1)$|\n",
    "|$a^{(3)} = g(\\Theta^{(2)}.a^{(2)})$| $(1\\times 1) = (3\\times 1)^T*(3\\times 1)$|\n",
    "|$$\\downarrow$$| |\n",
    "|$h_{\\Theta}(x) = a^{(3)}$| $(1\\times 1)$|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Classification (neural network)\n",
    "lets consider the four possible outcomes as y:\n",
    "$\\\\y^{(i)} = \\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}, \\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix} \\\\ \\\\$\n",
    "\n",
    "$NN \\implies \\begin{bmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\dots \\\\ x_n \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(2)} \\\\ a_1^{(2)} \\\\ a_2^{(2)} \\\\ \\dots \\end{bmatrix} \\rightarrow \\begin{bmatrix} a_0^{(3)} \\\\ a_1^{(3)} \\\\ a_2^{(3)} \\\\ \\dots \\end{bmatrix} \\rightarrow \\dots \\rightarrow \\begin{bmatrix} h_{\\Theta}(x)_1 \\\\ h_{\\Theta}(x)_2 \\\\ h_{\\Theta}(x)_3 \\\\ h_{\\Theta}(x)_4 \\end{bmatrix} \\\\ \\\\$  \n",
    "  \n",
    "i.e., if $h_{\\Theta}(x) = \\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0  \\end{bmatrix} \\implies $ output as class 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\4L neural network.drawio.svg\">  \n",
    "  \n",
    "  Fig: 4 Layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let training set be:  \n",
    "    \n",
    "$\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$  \n",
    "$L = $ total no. of layers $(l)$ in network  \n",
    "$s_l = $ no. of units(not counting bias unit) in layer $l$   \n",
    "i.e., in above nn $s_1 = 3, s_2 = 5, s_3 = 5, s_4 = s_L = 4$  \n",
    "  \n",
    "**Binary Classification:**  \n",
    "$y =$ 0 or 1  \n",
    "i.e., 1 output unit  \n",
    "  \n",
    "**Multi-class classification ($K$ classes)**  \n",
    "$y \\text{ }\\large{\\epsilon} \\text{ } \\mathbb{R}^K$  \n",
    "e.g., $\\underbrace{\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{pedestrain}, \\underbrace{\\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{bmatrix}}_{car}, \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{bmatrix}}_{motorcycle}, \\underbrace{\\begin{bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{bmatrix}}_{truck}$  \n",
    "  \n",
    "$K$ output units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function  \n",
    "### Logistic Regression  \n",
    "$\\large{J(\\theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\bigg(y^{(i)}\\log(h_{\\theta}(x^{(i)})) + (1-y^{(i)})\\log(1-h_{\\theta}(x^{(i)}))\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{j=1}^n\\theta_j^2}$  \n",
    "  \n",
    "### Neurla network:  \n",
    "$$\\large{h_{\\Theta}(x) \\text{ } \\Large{\\epsilon} \\text{ } \\mathbb{R}^{K} \\text{~~~~~} (h_{\\Theta}(x))_i = i^{th} output}$$  \n",
    "$\\large{J(\\Theta) = \\bigg[- \\displaystyle\\frac{1}{m}\\displaystyle\\sum_{i=1}^{m}\\sum_{k=1}^K \\bigg(y_k^{(i)}\\log(h_{\\Theta}(x^{(i)}))_k + (1-y_k^{(i)}) \\log(1-h_{\\Theta}(x^{(i)}))_k\\bigg)\\bigg] + \\displaystyle\\frac{\\lambda}{2m}\\displaystyle\\sum_{l=1}^{L-1}\\displaystyle\\sum_{i=1}^{s_l}\\displaystyle\\sum_{j=1}^{s_{l+1}}(\\Theta_{ji}^{(l)})^2}$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propogation Algorithm  \n",
    "  \n",
    "training set: $\\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),\\dots,(x^{(m)},y^{(m)})\\}$  \n",
    "* set $\\Delta^{(l)}_{i,j} := 0 \\text{ }\\forall (l,i,j)$  \n",
    "  \n",
    "for t = 1 to m:    \n",
    "1. set $a^{(1)} := x^{(t)}$  \n",
    "2. compute $a^{(l)}$ for $l=2,3,...,L$  \n",
    "    * for one training example $(x,y)$\n",
    "      * $a^{(1)} = x$  \n",
    "      * $z^{(2)} = \\Theta^{(1)}a^{(1)}$  \n",
    "      * $a^{(2)} = g(z^{(2)})  \\text{~~~~~~~(add } a_0^{(2)})$\n",
    "      * $z^{(3)} = \\Theta^{(2)}a^{(2)}$  \n",
    "      * $a^{(3)} = g(z^{(3)})  \\text{~~~~~~~(add } a_0^{(3)})$\n",
    "      * $z^{(4)} = \\Theta^{(3)}a^{(3)}$  \n",
    "      * $a^{(4)} = h_{\\Theta}(x) = g(z^{(2)})$\n",
    "3. $\\delta^{(L)} = a^{(L)} - y^{(t)}$\n",
    "4. Compute $\\delta^{(L-1)}, \\delta^{(L-2)},...,\\delta^{(2)}$ using $\\large{\\delta^{(l)} = ((\\Theta^{(l)})^T\\delta^{(l+1)}).*\\underbrace{z^{(l)}.*(1-z^{(l)})}_{g'(z^{(l)})}}  \\text{~~~~~~~~~~(add } z_0^{(l)} \\text{ to } z^{(l)}) $  \n",
    "$\\because g'(x) = x.*(1-x)$  \n",
    "  \n",
    "5. $\\Delta^{(l)}_{i,j} := \\Delta^{(l)}_{i,j} + a_j^{(l)}\\delta_i^{(l+1)}$, in vector form $\\Delta^{(l)} := \\Delta^{(l)}+\\delta^{(l+1)}(a^{(l)})^T$  \n",
    "  \n",
    "final $\\Delta$ matrix will be:  \n",
    "  * $D_{i,j}^{(l)}:= \\displaystyle\\frac{1}{m}(\\Delta_{i,j}^{(l)}+\\lambda\\Theta_{i,j}^{(l)}),$ if $j\\ne0$  \n",
    "  * $D_{i,j}^{(l)}:= \\displaystyle\\frac{1}{m}\\Delta_{i,j}^{(l)}$ if $j=0$  \n",
    "  \n",
    "i.e., $\\frac{\\delta}{\\delta\\Theta_{ij}^{(l)}}J(\\Theta) = D_{i,j}^{(l)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging a Learning algorithm  \n",
    "* Get more training examples\n",
    "* Try smaller sets of features\n",
    "* Try getting additional features\n",
    "* Try adding polynomial features $(x_1^2, x_2^2, x_1x_2,..ets.)$  \n",
    "* Try decreasing $\\lambda$\n",
    "* Try increasing $\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide data into 3 sets\n",
    "* Training set $\\text{~~~~~~~~~~~~~~~~~~~~~~}x_{train},y_{train}$\n",
    "* Cross-validation set $\\text{~~~~~~~~~~}x_{cv},y_{cv}$\n",
    "* Test set $\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~}x_{test},y_{test}$\n",
    "\n",
    "in general its ratio is 60% - 20% - 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The test set error\n",
    "  \n",
    "$J_{test}(\\Theta) = \\displaystyle\\frac{1}{2m_{test}}\\displaystyle\\sum_{i=1}^{m_{test}}(h_{\\Theta}(x_{test}^{(i)})-y_{test}^{(i)})^{2}$\n",
    "  \n",
    "$err(h_{\\theta}(x),y) =\\begin{cases} \n",
    "    1 & \\begin{cases} \\text{if~~} h_{\\theta}(x) \\ge 0.5 \\text{ \\& } y=0 \\\\ & \\text{or} \\\\ \\text{if ~}h_{\\theta}(x) < 0.5 \\text{ \\& } y=1\\end{cases}\\\\ \\\\\n",
    "    0 & \\text{otherwise}\n",
    "\\end{cases}$  \n",
    "  \n",
    "  \n",
    "$\\text{Test Error} = \\displaystyle\\frac{1}{m_{test}}\\sum_{i=1}^{m_{test}}err(h_{\\theta}(x_{test}^{(i)}),y_{test}^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection (degree of polynomial)\n",
    "$\\begin{matrix} d=1 & h_{\\theta}(x) = \\theta_0 + \\theta_1x & \\rightarrow \\theta^{(1)} \\rightarrow J_{test}(\\theta^{(1)})\\\\\n",
    "d=2 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 & \\rightarrow \\theta^{(2)} \\rightarrow J_{test}(\\theta^{(2)})\\\\\n",
    "d=3 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\dots + \\theta_3x^3 & \\rightarrow \\theta^{(3)} \\rightarrow J_{test}(\\theta^{(3)})\\\\\n",
    "\\vdots & &\\\\\n",
    "d=10 & h_{\\theta}(x) = \\theta_0 + \\theta_1x + \\dots + \\theta_{10}x^{10} & \\rightarrow \\theta^{(10)} \\rightarrow J_{test}(\\theta^{(10)})\n",
    "\\end{matrix}$\n",
    "  \n",
    "$d$ is degree of polynomial\n",
    "\n",
    "tune the $d$ parameter using the cross validataion dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"media\\bias-variance.drawio.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try different values of $\\lambda \\text{~~}\\epsilon \\text{~~}\\{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\\}$  \n",
    "tune $\\lambda$ using cross validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "$J_{train}(\\Theta) = \\displaystyle\\frac{1}{2m_{train}}\\displaystyle\\sum_{i=1}^{m_{train}}(h_{\\Theta}(x_{train}^{(i)})-y_{train}^{(i)})^{2}$  \n",
    "  \n",
    "$J_{cv}(\\Theta) = \\displaystyle\\frac{1}{2m_{cv}}\\displaystyle\\sum_{i=1}^{m_{cv}}(h_{\\Theta}(x_{cv}^{(i)})-y_{cv}^{(i)})^{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\Learning curves.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Bias**:  \n",
    "* **Low training set size:** causes $J_{train}(\\Theta)$ to be low and $J_{CV}(\\Theta)$ to be high\n",
    "* **Large training set size:** causes $J_{train}(\\Theta)$ and $J_{CV}(\\Theta)$ to be high with $J_{train}(\\Theta)\\approx J_{CV}(\\Theta)$  \n",
    "  \n",
    "If a learning algorithm is suffering from **high bias**, getting more training data will not (**by itself**) help much.  \n",
    "  \n",
    "**High Varience**: \n",
    "* **Low training set size:** $J_{train}(\\Theta)$ will be low and $J_{CV}(\\Theta)$ will be high\n",
    "* **Large training set size**: $J_{train}(\\Theta)$ increases with training set size and $J_{CV}(\\Theta)$ continues to decrease without leveling off. Also, $J_{train}(\\Theta) < J_{CV}(\\Theta)$ but the difference between them remains significant.  \n",
    "  \n",
    "If a learning algorithm is suffering from **high variance**, getting more training data is likely to help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "bl { color: blue}\n",
    "</style>\n",
    "\n",
    "## $\\therefore$\n",
    "* Get more training examples $\\rightarrow$ <g>fixes high varience</g>\n",
    "* Try smaller sets of features $\\rightarrow$ <g>fixes high varience</g>\n",
    "* Try getting additional features $\\rightarrow$ <g>fixes high bias</g>\n",
    "* Try adding polynomial features $(x_1^2, x_2^2, x_1x_2,..ets.)$  $\\rightarrow$  <g>fixes high bias</g>\n",
    "* Try decreasing $\\lambda$ $\\rightarrow$  <g>fixes high bias</g>\n",
    "* Try increasing $\\lambda$ $\\rightarrow$  <g>fixes high varience</g>\n",
    "  \n",
    "**Effects of no. of parameters** (no. of layers or no. of neurons) \n",
    "* A neural network with <bl>fewer parameters</bl> is **likely to underfit**. It also **computationally cheaper**\n",
    "* A neural network with <bl>many parameters</bl> is **likely to overfit**. which is **computationally expensive**. using <g>lower values of $\\lambda$ may address the issue</g>\n",
    "  \n",
    "**Effects of model complexity**\n",
    "* **Low order polynomials** (low model complexity) have <r>high bias</r> and <r>low variance</r>. In this case model fits <bl>poorly consistently</bl>.\n",
    "* **Higher-order polynomials** (high model complexity) <g>fit training data</g> <bl>extremely well</bl> and the <g>test data</g> <bl>extremely poor</bl>.   i.e., <r>low bias</r> & <r>high variance</r> on training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative view of logistic regression  \n",
    "Cost of example:   \n",
    "$\\text{~~~~~~~~~~~~~~~~}-(y\\log h_{\\theta}(x)+(1-y)\\log(1-h_{\\theta}(x)))$  \n",
    "  \n",
    "$\\text{~~~~~~~~~~~~~~~~}=-y\\log\\displaystyle\\frac{1}{1+e^{-\\theta^Tx}}-(1-y)\\log(1-\\displaystyle\\frac{1}{1+e^{-\\theta^Tx}})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\log function.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "r { color: Red }\n",
    "o { color: Orange }\n",
    "g { color: Green }\n",
    "bl { color: blue}\n",
    "</style>\n",
    "# Support Vector machine\n",
    "<bl>Logistic regression:</bl>  \n",
    "  \n",
    "$\\min\\limits_{\\theta}\\displaystyle\\frac 1m\\bigg[\\sum_{i=1}^{m}y^{(i)}\\underbrace{\\bigg(-\\log h_{\\theta}(x^{(i)})\\bigg)}_{Cost_1(\\theta^Tx^{(i)})}+(1-y^{(i)})\\underbrace{\\bigg((-\\log(1-h_{\\theta}(x^{(i)}))\\bigg)}_{Cost_0(\\theta^Tx^{(i)})}\\bigg]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta_j^2$  \n",
    "  \n",
    "<bl>Support vector machine:</bl>  \n",
    "   \n",
    "eleminating constant <r>$m$</r>  \n",
    "$\\min\\limits_{\\theta}\\displaystyle\\xcancel{\\frac{1}{m}}\\bigg[\\sum_{i=1}^{m}y^{(i)}Cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\\theta^Tx^{(i)})\\bigg]+\\frac{\\lambda}{2\\xcancel m}\\sum_{j=1}^n\\theta_j^2$  \n",
    "$\\text{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}\\downarrow\\downarrow\\downarrow$  \n",
    "  \n",
    "also eleminating <r>$\\lambda$</r> as:  \n",
    "$\\min\\limits_{\\theta}$ A+$\\lambda$ B $\\text{~~~~~~~~~~~}$ can be writtent as $\\text{~~~~~~~~~~~}$ $\\min\\limits_{\\theta}$ CA+B  \n",
    "where $C \\propto \\displaystyle\\frac 1\\lambda$  \n",
    "  \n",
    "$\\implies \\min\\limits_{\\theta}C\\displaystyle\\bigg[\\sum_{i=1}^{m}y^{(i)}Cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})Cost_0(\\theta^Tx^{(i)})\\bigg]+\\frac{1}{2}\\sum_{j=1}^n\\theta_j^2$  \n",
    "  \n",
    "<bl>SVM hypothesis</bl>  \n",
    "  \n",
    "$h_{\\theta}(x) = \\begin{cases} 1 & \\text{if~~} \\theta^Tx \\geq 0\\\\\n",
    "                                0 & \\text{otherwise}\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='media\\SVM.drawio.svg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Decision Boundary with outliers\n",
    "<img src='media\\svm with outliers.drawio.svg'>\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "57baa5815c940fdaff4d14510622de9616cae602444507ba5d0b6727c008cbd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
